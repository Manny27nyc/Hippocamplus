---
title: tSNE and clustering
date: 2018-02-13
tags: ["R", "stats"]
---



<p>tSNE can give really nice results when we want to visualize many groups of multi-dimensional points. Once the 2D graph is done we might want to identify which points cluster in the tSNE blobs.</p>
<p>Using simulated and real data, I’ll try different methods:</p>
<ul>
<li>Hierarchical clustering</li>
<li>K-means</li>
<li>Gaussian mixture</li>
<li>Density-based clustering</li>
<li>Louvain community detection.</li>
</ul>
<p><strong>TL;DR</strong> If &lt;30K points, hierarchical clustering is robust, easy to use and with reasonable computing time. KNN + Louvain is fast and works well in general.</p>
<hr />
<pre class="r"><code>library(ggplot2)
library(dplyr)
library(magrittr)
library(ggrepel)</code></pre>
<div id="data" class="section level2">
<h2>Data</h2>
<div id="normally-distributed-points" class="section level3">
<h3>Normally distributed points</h3>
<p>First, I’ll simulate an easy situation with 10 different groups. 5,000 points are distributed following Gaussian distributions in 100 dimensions. Points are randomly assigned a group. For each group, 3 dimensions are randomly selected and the points shifted.</p>
<p>Because there are 10 groups that differ in different dimensions, a PCA shouldn’t be able to separate all the groups with the first two components. That’s when the tSNE comes in to do its magic (easily).</p>
<pre class="r"><code>set.seed(123456)
N = 5000
D = 100
data.norm = matrix(rnorm(N * D, 2), N)
groups.probs = runif(10)
groups = sample(1:10, N, TRUE, groups.probs/sum(groups.probs))
for (gp in unique(groups)) {
    dev = rep(1, D)
    dev[sample.int(D, 3)] = runif(3, -10, 10)
    data.norm[which(groups == gp), ] = data.norm[which(groups == 
        gp), ] %*% diag(dev)
}
info.norm = tibble(truth = factor(groups))</code></pre>
<p>The PCA and tSNE look like this:</p>
<pre class="r"><code>pca.norm = prcomp(data.norm)
info.norm %&lt;&gt;% cbind(pca.norm$x[, 1:4])
ggplot(info.norm, aes(x = PC1, y = PC2, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/pcatsne-1.png" width="768" /></p>
<pre class="r"><code>ggplot(info.norm, aes(x = PC3, y = PC4, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/pcatsne-2.png" width="768" /></p>
<p>We see something but it’s not so clear, let’s run the tSNE.</p>
<pre class="r"><code>library(Rtsne)
tsne.norm = Rtsne(pca.norm$x, pca = FALSE)
info.norm %&lt;&gt;% mutate(tsne1 = tsne.norm$Y[, 1], tsne2 = tsne.norm$Y[, 
    2])
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/tsne-1.png" width="768" /></p>
<p><em>Time for this code chunk: 83.76 s</em></p>
</div>
<div id="real-data" class="section level3">
<h3>Real data</h3>
<p>As a real-life example, I use the data that motivated this exploration. It contains a bit more than 26K points and the tSNE looks like that:</p>
<pre class="r"><code>tsne.real = read.csv(&quot;https://docs.google.com/uc?id=1KArwfOd5smzuCsrpgW9Xpf9I06VOW4ga&amp;export=download&quot;)
info.real = tsne.real
ggplot(tsne.real, aes(x = tsne1, y = tsne2)) + geom_point(alpha = 0.1) + 
    theme_bw()</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/real-1.png" width="768" /></p>
</div>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2>Hierarchical clustering</h2>
<ul>
<li><strong>+</strong> Once built, it’s fast to try different number clusters.</li>
<li><strong>+</strong> Different linkage criteria to match the behavior we want.</li>
<li><strong>-</strong> Doesn’t scale well. High memory usage and computation time when &gt;30K.</li>
</ul>
<pre class="r"><code>hc.norm = hclust(dist(tsne.norm$Y))
info.norm$hclust = factor(cutree(hc.norm, 9))
hc.norm.cent = info.norm %&gt;% group_by(hclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.norm.cent) + guides(colour = FALSE) + 
    ggtitle(&quot;Linkage criterion: Complete&quot;)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/hcnorm-1.png" width="768" /></p>
<pre class="r"><code>hc.norm = hclust(dist(tsne.norm$Y), method = &quot;ward.D&quot;)
info.norm$hclust = factor(cutree(hc.norm, 9))
hc.norm.cent = info.norm %&gt;% group_by(hclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.norm.cent) + guides(colour = FALSE) + 
    ggtitle(&quot;Linkage criterion: Ward&quot;)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/hcnorm-2.png" width="768" /></p>
<p>Now on real data:</p>
<pre class="r"><code>hc.real = hclust(dist(tsne.real))
info.real$hclust = factor(cutree(hc.real, 18))
hc.real.cent = info.real %&gt;% group_by(hclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.real.cent) + guides(colour = FALSE) + 
    ggtitle(&quot;Linkage criterion: Complete&quot;)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/hcreal-1.png" width="768" /></p>
<pre class="r"><code>hc.real = hclust(dist(tsne.real), method = &quot;ward.D&quot;)
info.real$hclust = factor(cutree(hc.real, 18))
hc.real.cent = info.real %&gt;% group_by(hclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.real.cent) + guides(colour = FALSE) + 
    ggtitle(&quot;Linkage criterion: Ward&quot;)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/hcreal2-1.png" width="768" /></p>
<p><em>Time for this code chunk: 38.01 s</em></p>
<p>For both data, Ward gives the best clusters. For example it splits the top-left clusters better in the real data.</p>
</div>
<div id="kmeans" class="section level2">
<h2>Kmeans</h2>
<ul>
<li><strong>+</strong> Very fast.</li>
<li><strong>-</strong> Simple.</li>
</ul>
<pre class="r"><code>km.norm = kmeans(tsne.norm$Y, 9, nstart = 100)
info.norm$kmeans = factor(km.norm$cluster)
km.cent = info.norm %&gt;% group_by(kmeans) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE) + ggtitle(&quot;9 clusters&quot;)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/kmnorm-1.png" width="768" /></p>
<pre class="r"><code>km.norm = kmeans(tsne.norm$Y, 10, nstart = 100)
info.norm$kmeans = factor(km.norm$cluster)
km.cent = info.norm %&gt;% group_by(kmeans) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE) + ggtitle(&quot;10 clusters&quot;)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/kmnorm-2.png" width="768" /></p>
<p>Because it’s not working well for cluster that are not “round”, we need to ask for more clusters. In practice we’ll need to merge back together the clusters that were fragmented.</p>
<pre class="r"><code>set.seed(123456)
km.real = kmeans(tsne.real, 24, nstart = 200, iter.max = 100)
info.real$kmeans = factor(km.real$cluster)
km.cent = info.real %&gt;% group_by(kmeans) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/kmreal-1.png" width="768" /></p>
<p>Not perfect in the middle-left big cluster: cluster 11 is grabbing points from the bottom blob. Maybe increasing the number of clusters could fix this?</p>
<pre class="r"><code>set.seed(123456)
km.real = kmeans(tsne.real, 25, nstart = 200, iter.max = 100)
info.real$kmeans = factor(km.real$cluster)
km.cent = info.real %&gt;% group_by(kmeans) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/kmrealmore-1.png" width="768" /></p>
<p><em>Time for this code chunk: 15.48 s</em></p>
<p>Better. Same as with the other methods: we need to manually tweak the parameters to obtain the clustering we want…</p>
<p>Note: using several starting points help getting more robust results (<code>nstart=</code>). Increasing the number of iterations helps too (<code>iter.max=</code>).</p>
</div>
<div id="mclust" class="section level2">
<h2>Mclust</h2>
<ul>
<li><strong>+</strong> Better clusters.</li>
<li><strong>+</strong> Can find the best K (number of clusters (although slowly).</li>
<li><strong>-</strong> Slow.</li>
<li><strong>-</strong> Need to be recomputed for each choice of K (number of clusters).</li>
</ul>
<pre class="r"><code>library(mclust)
mc.norm = Mclust(tsne.norm$Y, 9)
info.norm$mclust = factor(mc.norm$classification)
mc.cent = info.norm %&gt;% group_by(mclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/mcnorm-1.png" width="768" /></p>
<p>Even the elongated cluster is nicely identified and we don’t need to split it.</p>
<pre class="r"><code>set.seed(123456)
mc.real = Mclust(tsne.real, 20, initialization = list(subset = sample.int(nrow(tsne.real), 
    1000)))
info.real$mclust = factor(mc.real$classification)
mc.cent = info.real %&gt;% group_by(mclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/mcreal-1.png" width="768" /></p>
<p>Sometimes the results are a bit surprising. For example, points are assigned to cluster far away or there is another cluster in between (e.g. clusters 6 and 17). As usual changing the number of clusters helps.</p>
<pre class="r"><code>set.seed(123456)
mc.real = Mclust(tsne.real, 24, initialization = list(subset = sample.int(nrow(tsne.real), 
    1000)))
info.real$mclust = factor(mc.real$classification)
mc.cent = info.real %&gt;% group_by(mclust) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/mcreal2-1.png" width="768" /></p>
<p><em>Time for this code chunk: 90.61 s</em></p>
<p>Note: I had to use the sub-sampling trick to speed up the process, otherwise it was taking too long. Using <code>initialization=list(subset=sample.int(nrow(tsne.real), 1000))</code>, only a thousand points are used for the EM (but all the points are assigned to a cluster at the end).</p>
</div>
<div id="density-based-clustering" class="section level2">
<h2>Density-based clustering</h2>
<ul>
<li><strong>+</strong> Can find clusters with different “shapes”.</li>
<li><strong>-</strong> Bad on real/noisy data.</li>
<li><strong>-</strong> Slow when many points.</li>
</ul>
<pre class="r"><code>library(fpc)
ds.norm = dbscan(tsne.norm$Y, 2)
info.norm$density = factor(ds.norm$cluster)
ds.cent = info.norm %&gt;% group_by(density) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = density)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = density), 
    data = ds.cent) + guides(colour = FALSE)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/densnorm-1.png" width="768" /></p>
<p>Woah, it found the small cluster !</p>
<pre class="r"><code>ds.real = dbscan(tsne.real, 1)
info.real$density = factor(ds.real$cluster)
ds.cent = info.real %&gt;% group_by(density) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = density)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = density), 
    data = ds.cent) + guides(colour = FALSE)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/densreal-1.png" width="768" /></p>
<p><em>Time for this code chunk: 100.98 s</em></p>
<p>Ouch…</p>
</div>
<div id="knn-graph-and-louvain-community-detection" class="section level2">
<h2>KNN graph and Louvain community detection</h2>
<pre class="r"><code>library(igraph)
library(FNN)
k = 100
knn.norm = get.knn(as.matrix(tsne.norm$Y), k = k)
knn.norm = data.frame(from = rep(1:nrow(knn.norm$nn.index), 
    k), to = as.vector(knn.norm$nn.index), weight = 1/(1 + 
    as.vector(knn.norm$nn.dist)))
nw.norm = graph_from_data_frame(knn.norm, directed = FALSE)
nw.norm = simplify(nw.norm)
lc.norm = cluster_louvain(nw.norm)
info.norm$louvain = as.factor(membership(lc.norm))
lc.cent = info.norm %&gt;% group_by(louvain) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = louvain)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = louvain), 
    data = lc.cent) + guides(colour = FALSE)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/louvnorm-1.png" width="768" /></p>
<p>Playing with the resolution parameter we can get more/less communities. For <code>gamma=.3</code>:</p>
<pre class="r"><code>lc.norm = cluster_louvain(nw.norm, gamma = 0.3)
info.norm$louvain = as.factor(membership(lc.norm))
lc.cent = info.norm %&gt;% group_by(louvain) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = louvain)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = louvain), 
    data = lc.cent) + guides(colour = FALSE)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/gamma-1.png" width="768" /></p>
<p>On real data and using <code>gamma=.1</code>:</p>
<pre class="r"><code>k = 100
knn.real = get.knn(as.matrix(tsne.real), k = k)
knn.real = data.frame(from = rep(1:nrow(knn.real$nn.index), 
    k), to = as.vector(knn.real$nn.index), weight = 1/(1 + 
    as.vector(knn.real$nn.dist)))
nw.real = graph_from_data_frame(knn.real, directed = FALSE)
nw.real = simplify(nw.real)
lc.real = cluster_louvain(nw.real, gamma = 0.1)
info.real$louvain = as.factor(membership(lc.real))
lc.cent = info.real %&gt;% group_by(louvain) %&gt;% select(tsne1, 
    tsne2) %&gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = louvain)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = louvain), 
    data = lc.cent) + guides(colour = FALSE)</code></pre>
<p><img src="/post/2017-12-01-tSNEclustering_files/figure-html/louvreal-1.png" width="768" /></p>
<p><em>Time for this code chunk: 18.86 s</em></p>
<p>Pretty good.</p>
<p><em>PS:</em> I added the resolution parameter <code>gamma</code> in the <em>igraph</em> function for the Louvain clustering. While it was easy to change in the C code, compiling <em>igraph</em> from source was a pain. I couldn’t get it to work on OSX but I managed to install this modified version of igraph on Linux (see <a href="https://github.com/jmonlong/Hippocamplus/tree/config/R/rigraph_gammalouvain">instructions</a>).</p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>If not too many points or too many groups, <strong>hierarchical clustering</strong> might be enough. Especially with the Ward criterion, it worked well for both simulated and real data. Once the hierarchy is built, it’s fast to try different values for the number of clusters. Also, in the real data, I could get satisfactory results using a lower number of clusters than for the K-means (18 vs 25).</p>
<p><strong>If there are too many points</strong> (e.g. &gt;30K), hierarchical clustering might be too demanding and I would fall back to <strong>KNN+Louvain</strong>. It’s fast enough and the results are pretty good.</p>
<p>The more advanced methods are good to keep in mind if the points ever form diverse or unusual shapes.</p>
<p>I learned two <strong>tricks to improve the performance</strong> of the methods: increasing the number of iterations and starting points for the K-means, and sub-sampling for the EM clustering.</p>
<p>Clustering points from the tSNE is good to explore the groups that we visually see in the tSNE but <strong>if we want more meaningful clusters</strong> we could run these methods <strong>in the PC space directly</strong>. The KNN + Louvain community clustering, for example, is used in single cell sequencing analysis.</p>
</div>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hippocamplus </title>
    <link>/tags/r/</link>
    <language>en-us</language>
    <author></author>
    <rights>(C) 2018</rights>
    <updated>2018-02-13 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>tSNE and clustering</title>
          <link>/2018/02/13/tsne-and-clustering/</link>
          <pubDate>Tue, 13 Feb 2018 00:00:00 UTC</pubDate>
          <author></author>
          <guid>/2018/02/13/tsne-and-clustering/</guid>
          <description>&lt;p&gt;tSNE can give really nice results when we want to visualize many groups of multi-dimensional points. Once the 2D graph is done we might want to identify which points cluster in the tSNE blobs.&lt;/p&gt;
&lt;p&gt;Using simulated and real data, I’ll try different methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hierarchical clustering&lt;/li&gt;
&lt;li&gt;K-means&lt;/li&gt;
&lt;li&gt;Gaussian mixture&lt;/li&gt;
&lt;li&gt;Density-based clustering&lt;/li&gt;
&lt;li&gt;Louvain community detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; If &amp;lt;30K points, hierarchical clustering is robust, easy to use and with reasonable computing time. KNN + Louvain is fast and works well in general.&lt;/p&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr)
library(magrittr)
library(ggrepel)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;div id=&#34;normally-distributed-points&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Normally distributed points&lt;/h3&gt;
&lt;p&gt;First, I’ll simulate an easy situation with 10 different groups. 5,000 points are distributed following Gaussian distributions in 100 dimensions. Points are randomly assigned a group. For each group, 3 dimensions are randomly selected and the points shifted.&lt;/p&gt;
&lt;p&gt;Because there are 10 groups that differ in different dimensions, a PCA shouldn’t be able to separate all the groups with the first two components. That’s when the tSNE comes in to do its magic (easily).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
N = 5000
D = 100
data.norm = matrix(rnorm(N * D, 2), N)
groups.probs = runif(10)
groups = sample(1:10, N, TRUE, groups.probs/sum(groups.probs))
for (gp in unique(groups)) {
    dev = rep(1, D)
    dev[sample.int(D, 3)] = runif(3, -10, 10)
    data.norm[which(groups == gp), ] = data.norm[which(groups == 
        gp), ] %*% diag(dev)
}
info.norm = tibble(truth = factor(groups))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The PCA and tSNE look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca.norm = prcomp(data.norm)
info.norm %&amp;lt;&amp;gt;% cbind(pca.norm$x[, 1:4])
ggplot(info.norm, aes(x = PC1, y = PC2, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/pcatsne-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(info.norm, aes(x = PC3, y = PC4, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/pcatsne-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see something but it’s not so clear, let’s run the tSNE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rtsne)
tsne.norm = Rtsne(pca.norm$x, pca = FALSE)
info.norm %&amp;lt;&amp;gt;% mutate(tsne1 = tsne.norm$Y[, 1], tsne2 = tsne.norm$Y[, 
    2])
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/tsne-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 83.76 s&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;real-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Real data&lt;/h3&gt;
&lt;p&gt;As a real-life example, I use the data that motivated this exploration. It contains a bit more than 26K points and the tSNE looks like that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tsne.real = read.csv(&amp;quot;https://docs.google.com/uc?id=1KArwfOd5smzuCsrpgW9Xpf9I06VOW4ga&amp;amp;export=download&amp;quot;)
info.real = tsne.real
ggplot(tsne.real, aes(x = tsne1, y = tsne2)) + geom_point(alpha = 0.1) + 
    theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/real-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Once built, it’s fast to try different number clusters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Different linkage criteria to match the behavior we want.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Doesn’t scale well. High memory usage and computation time when &amp;gt;30K.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc.norm = hclust(dist(tsne.norm$Y))
info.norm$hclust = factor(cutree(hc.norm, 9))
hc.norm.cent = info.norm %&amp;gt;% group_by(hclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.norm.cent) + guides(colour = FALSE) + 
    ggtitle(&amp;quot;Linkage criterion: Complete&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/hcnorm-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc.norm = hclust(dist(tsne.norm$Y), method = &amp;quot;ward.D&amp;quot;)
info.norm$hclust = factor(cutree(hc.norm, 9))
hc.norm.cent = info.norm %&amp;gt;% group_by(hclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.norm.cent) + guides(colour = FALSE) + 
    ggtitle(&amp;quot;Linkage criterion: Ward&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/hcnorm-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now on real data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc.real = hclust(dist(tsne.real))
info.real$hclust = factor(cutree(hc.real, 18))
hc.real.cent = info.real %&amp;gt;% group_by(hclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.real.cent) + guides(colour = FALSE) + 
    ggtitle(&amp;quot;Linkage criterion: Complete&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/hcreal-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc.real = hclust(dist(tsne.real), method = &amp;quot;ward.D&amp;quot;)
info.real$hclust = factor(cutree(hc.real, 18))
hc.real.cent = info.real %&amp;gt;% group_by(hclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.real.cent) + guides(colour = FALSE) + 
    ggtitle(&amp;quot;Linkage criterion: Ward&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/hcreal2-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 38.01 s&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For both data, Ward gives the best clusters. For example it splits the top-left clusters better in the real data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kmeans&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Kmeans&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Very fast.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Simple.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km.norm = kmeans(tsne.norm$Y, 9, nstart = 100)
info.norm$kmeans = factor(km.norm$cluster)
km.cent = info.norm %&amp;gt;% group_by(kmeans) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE) + ggtitle(&amp;quot;9 clusters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/kmnorm-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km.norm = kmeans(tsne.norm$Y, 10, nstart = 100)
info.norm$kmeans = factor(km.norm$cluster)
km.cent = info.norm %&amp;gt;% group_by(kmeans) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE) + ggtitle(&amp;quot;10 clusters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/kmnorm-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because it’s not working well for cluster that are not “round”, we need to ask for more clusters. In practice we’ll need to merge back together the clusters that were fragmented.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
km.real = kmeans(tsne.real, 24, nstart = 200, iter.max = 100)
info.real$kmeans = factor(km.real$cluster)
km.cent = info.real %&amp;gt;% group_by(kmeans) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/kmreal-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not perfect in the middle-left big cluster: cluster 11 is grabbing points from the bottom blob. Maybe increasing the number of clusters could fix this?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
km.real = kmeans(tsne.real, 25, nstart = 200, iter.max = 100)
info.real$kmeans = factor(km.real$cluster)
km.cent = info.real %&amp;gt;% group_by(kmeans) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/kmrealmore-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 15.48 s&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Better. Same as with the other methods: we need to manually tweak the parameters to obtain the clustering we want…&lt;/p&gt;
&lt;p&gt;Note: using several starting points help getting more robust results (&lt;code&gt;nstart=&lt;/code&gt;). Increasing the number of iterations helps too (&lt;code&gt;iter.max=&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mclust&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mclust&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Better clusters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Can find the best K (number of clusters (although slowly).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Slow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Need to be recomputed for each choice of K (number of clusters).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mclust)
mc.norm = Mclust(tsne.norm$Y, 9)
info.norm$mclust = factor(mc.norm$classification)
mc.cent = info.norm %&amp;gt;% group_by(mclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/mcnorm-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even the elongated cluster is nicely identified and we don’t need to split it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
mc.real = Mclust(tsne.real, 20, initialization = list(subset = sample.int(nrow(tsne.real), 
    1000)))
info.real$mclust = factor(mc.real$classification)
mc.cent = info.real %&amp;gt;% group_by(mclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/mcreal-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sometimes the results are a bit surprising. For example, points are assigned to cluster far away or there is another cluster in between (e.g. clusters 6 and 17). As usual changing the number of clusters helps.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
mc.real = Mclust(tsne.real, 24, initialization = list(subset = sample.int(nrow(tsne.real), 
    1000)))
info.real$mclust = factor(mc.real$classification)
mc.cent = info.real %&amp;gt;% group_by(mclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/mcreal2-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 90.61 s&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Note: I had to use the sub-sampling trick to speed up the process, otherwise it was taking too long. Using &lt;code&gt;initialization=list(subset=sample.int(nrow(tsne.real), 1000))&lt;/code&gt;, only a thousand points are used for the EM (but all the points are assigned to a cluster at the end).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-based-clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Density-based clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Can find clusters with different “shapes”.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Bad on real/noisy data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Slow when many points.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fpc)
ds.norm = dbscan(tsne.norm$Y, 2)
info.norm$density = factor(ds.norm$cluster)
ds.cent = info.norm %&amp;gt;% group_by(density) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = density)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = density), 
    data = ds.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/densnorm-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Woah, it found the small cluster !&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ds.real = dbscan(tsne.real, 1)
info.real$density = factor(ds.real$cluster)
ds.cent = info.real %&amp;gt;% group_by(density) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = density)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = density), 
    data = ds.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/densreal-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 100.98 s&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ouch…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;knn-graph-and-louvain-community-detection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;KNN graph and Louvain community detection&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(igraph)
library(FNN)
k = 100
knn.norm = get.knn(as.matrix(tsne.norm$Y), k = k)
knn.norm = data.frame(from = rep(1:nrow(knn.norm$nn.index), 
    k), to = as.vector(knn.norm$nn.index), weight = 1/(1 + 
    as.vector(knn.norm$nn.dist)))
nw.norm = graph_from_data_frame(knn.norm, directed = FALSE)
nw.norm = simplify(nw.norm)
lc.norm = cluster_louvain(nw.norm)
info.norm$louvain = as.factor(membership(lc.norm))
lc.cent = info.norm %&amp;gt;% group_by(louvain) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = louvain)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = louvain), 
    data = lc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/louvnorm-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Playing with the resolution parameter we can get more/less communities. For &lt;code&gt;gamma=.3&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lc.norm = cluster_louvain(nw.norm, gamma = 0.3)
info.norm$louvain = as.factor(membership(lc.norm))
lc.cent = info.norm %&amp;gt;% group_by(louvain) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = louvain)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = louvain), 
    data = lc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/gamma-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On real data and using &lt;code&gt;gamma=.1&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k = 100
knn.real = get.knn(as.matrix(tsne.real), k = k)
knn.real = data.frame(from = rep(1:nrow(knn.real$nn.index), 
    k), to = as.vector(knn.real$nn.index), weight = 1/(1 + 
    as.vector(knn.real$nn.dist)))
nw.real = graph_from_data_frame(knn.real, directed = FALSE)
nw.real = simplify(nw.real)
lc.real = cluster_louvain(nw.real, gamma = 0.1)
info.real$louvain = as.factor(membership(lc.real))
lc.cent = info.real %&amp;gt;% group_by(louvain) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = louvain)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = louvain), 
    data = lc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/louvreal-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 18.86 s&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Pretty good.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PS:&lt;/em&gt; I added the resolution parameter &lt;code&gt;gamma&lt;/code&gt; in the &lt;em&gt;igraph&lt;/em&gt; function for the Louvain clustering. While it was easy to change in the C code, compiling &lt;em&gt;igraph&lt;/em&gt; from source was a pain. I couldn’t get it to work on OSX but I managed to install this modified version of igraph on Linux (see &lt;a href=&#34;https://github.com/jmonlong/Hippocamplus/tree/config/R/rigraph_gammalouvain&#34;&gt;instructions&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;If not too many points or too many groups, &lt;strong&gt;hierarchical clustering&lt;/strong&gt; might be enough. Especially with the Ward criterion, it worked well for both simulated and real data. Once the hierarchy is built, it’s fast to try different values for the number of clusters. Also, in the real data, I could get satisfactory results using a lower number of clusters than for the K-means (18 vs 25).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If there are too many points&lt;/strong&gt; (e.g. &amp;gt;30K), hierarchical clustering might be too demanding and I would fall back to &lt;strong&gt;KNN+Louvain&lt;/strong&gt;. It’s fast enough and the results are pretty good.&lt;/p&gt;
&lt;p&gt;The more advanced methods are good to keep in mind if the points ever form diverse or unusual shapes.&lt;/p&gt;
&lt;p&gt;I learned two &lt;strong&gt;tricks to improve the performance&lt;/strong&gt; of the methods: increasing the number of iterations and starting points for the K-means, and sub-sampling for the EM clustering.&lt;/p&gt;
&lt;p&gt;Clustering points from the tSNE is good to explore the groups that we visually see in the tSNE but &lt;strong&gt;if we want more meaningful clusters&lt;/strong&gt; we could run these methods &lt;strong&gt;in the PC space directly&lt;/strong&gt;. The KNN + Louvain community clustering, for example, is used in single cell sequencing analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>MUMmerplots with ggplot2</title>
          <link>/2017/09/19/mummerplots-with-ggplot2/</link>
          <pubDate>Tue, 19 Sep 2017 00:00:00 UTC</pubDate>
          <author></author>
          <guid>/2017/09/19/mummerplots-with-ggplot2/</guid>
          <description>&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(magrittr)
library(GenomicRanges)
library(knitr)
library(ggplot2)
library(tidyr)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;mummer-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MUMmer plot&lt;/h2&gt;
&lt;p&gt;The MUMmer plot that I want to reproduce showed three contigs overlapping a region of chr 14. I had filtered the delta file with &lt;code&gt;delta-filter -l 10000 -q -r&lt;/code&gt; to get only the contigs with the best alignments. I had used &lt;code&gt;mummerplot&lt;/code&gt; with the &lt;code&gt;-l&lt;/code&gt; layout option to reorder and orient the sequences to have a nice diagonal.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../imgs/mumplot-example.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;read-a-delta-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Read a delta file&lt;/h2&gt;
&lt;p&gt;The delta file is the default output of the &lt;a href=&#34;http://mummer.sourceforge.net/manual/#nucmer&#34;&gt;NUCmer alignment script&lt;/a&gt;. The format of the delta file is described more &lt;a href=&#34;http://mummer.sourceforge.net/manual/#nucmeroutput&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;readDelta &amp;lt;- function(deltafile) {
    lines = scan(deltafile, &amp;quot;a&amp;quot;, sep = &amp;quot;\n&amp;quot;, quiet = TRUE)
    lines = lines[-1]
    lines.l = strsplit(lines, &amp;quot; &amp;quot;)
    lines.len = lapply(lines.l, length) %&amp;gt;% as.numeric
    lines.l = lines.l[lines.len != 1]
    lines.len = lines.len[lines.len != 1]
    head.pos = which(lines.len == 4)
    head.id = rep(head.pos, c(head.pos[-1], length(lines.l) + 
        1) - head.pos)
    mat = matrix(as.numeric(unlist(lines.l[lines.len == 
        7])), 7)
    res = as.data.frame(t(mat[1:5, ]))
    colnames(res) = c(&amp;quot;rs&amp;quot;, &amp;quot;re&amp;quot;, &amp;quot;qs&amp;quot;, &amp;quot;qe&amp;quot;, &amp;quot;error&amp;quot;)
    res$qid = unlist(lapply(lines.l[head.id[lines.len == 
        7]], &amp;quot;[&amp;quot;, 2))
    res$strand = ifelse(res$qe - res$qs &amp;gt; 0, &amp;quot;+&amp;quot;, &amp;quot;-&amp;quot;)
    res
}

mumgp = readDelta(&amp;quot;../../data/mumplot-example.delta&amp;quot;)

mumgp %&amp;gt;% head %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rs&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;re&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;qs&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;qe&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;error&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;qid&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;strand&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;265577&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;265842&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;108520&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;108254&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Contig0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;265577&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;265842&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;106438&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;106172&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Contig0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;306695&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;306968&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;138241&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;138515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Contig0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1016956&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1017364&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27806&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27394&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;62&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Contig0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1723715&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1723990&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34123&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33845&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Contig0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1767531&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1767813&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33842&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34123&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Contig0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;+&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;filter-contigs-with-poor-alignments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Filter contigs with poor alignments&lt;/h2&gt;
&lt;p&gt;For now, I filter contigs simply based on the size of the aligned segment. I keep only contigs with at least one aligned segment larger than a minimum size. Smaller alignment in these contigs are kept if in the same range as the large aligned segments. Eventually, I could also filter segment based on the number/proportion of errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filterMum &amp;lt;- function(df, minl = 1000, flanks = 10000) {
    coord = df %&amp;gt;% filter(abs(re - rs) &amp;gt; minl) %&amp;gt;% 
        group_by(qid) %&amp;gt;% summarize(qsL = min(qs) - 
        flanks, qeL = max(qe) + flanks, rs = min(rs)) %&amp;gt;% 
        ungroup %&amp;gt;% arrange(desc(rs)) %&amp;gt;% mutate(qid = factor(qid, 
        levels = unique(qid))) %&amp;gt;% select(-rs)
    merge(df, coord) %&amp;gt;% filter(qs &amp;gt; qsL, qe &amp;lt; qeL) %&amp;gt;% 
        mutate(qid = factor(qid, levels = levels(coord$qid))) %&amp;gt;% 
        select(-qsL, -qeL)
}

mumgp.filt = filterMum(mumgp, minl = 10000)
mumgp.filt %&amp;gt;% head %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;qid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rs&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;re&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;qs&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;qe&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;error&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;strand&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Contig1475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1663946&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1665485&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;331648&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;330113&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;171&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Contig1475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1662200&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1684396&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;126037&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;103837&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;234&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Contig1475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1581333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1582738&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;244635&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;243233&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;87&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Contig1475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1597381&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1610746&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;145948&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;132626&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;157&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Contig1475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1610278&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1623358&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;130561&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;117468&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Contig1475&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1616542&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1618080&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;331648&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;330113&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;146&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;graph&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Graph&lt;/h2&gt;
&lt;p&gt;I’m going for the same style as &lt;code&gt;mummerplot&lt;/code&gt; to compare.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mumgp.filt, aes(x = rs, xend = re, y = qs, yend = qe, 
    colour = strand)) + geom_segment() + geom_point(alpha = 0.5) + 
    facet_grid(qid ~ ., scales = &amp;quot;free&amp;quot;, space = &amp;quot;free&amp;quot;, 
        switch = &amp;quot;y&amp;quot;) + theme_bw() + theme(strip.text.y = element_text(angle = 180, 
    size = 5), strip.background = element_blank(), 
    legend.position = c(0.99, 0.01), legend.justification = c(1, 
        0), axis.text.y = element_blank(), axis.ticks.y = element_blank()) + 
    xlab(&amp;quot;reference sequence&amp;quot;) + ylab(&amp;quot;assembly&amp;quot;) + 
    scale_colour_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-19-MumplotsWithGgplot2_files/figure-html/graph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not bad but it would look nicer if we flipped the contigs to have more or less a diagonal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diagonalize&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagonalize&lt;/h2&gt;
&lt;p&gt;For each contig, I compute the major strand (strand with most bases aligned) and flip if necessary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diagMum &amp;lt;- function(df) {
    major.strand = df %&amp;gt;% group_by(qid) %&amp;gt;% summarize(major.strand = ifelse(sum(sign(qe - 
        qs) * abs(qe - qs)) &amp;gt; 0, &amp;quot;+&amp;quot;, &amp;quot;-&amp;quot;), maxQ = max(c(qe, 
        qs)))
    merge(df, major.strand) %&amp;gt;% mutate(qs2 = ifelse(major.strand == 
        &amp;quot;-&amp;quot;, maxQ - qs, qs), qe2 = ifelse(major.strand == 
        &amp;quot;-&amp;quot;, maxQ - qe, qe), qs = qs2, qe = qe2) %&amp;gt;% 
        select(-qe2, -qs2, maxQ)
}

mumgp.filt.diag = diagMum(mumgp.filt)

ggplot(mumgp.filt.diag, aes(x = rs, xend = re, y = qs, 
    yend = qe, colour = strand)) + geom_segment() + 
    geom_point(alpha = 0.5) + facet_grid(qid ~ ., scales = &amp;quot;free&amp;quot;, 
    space = &amp;quot;free&amp;quot;, switch = &amp;quot;y&amp;quot;) + theme_bw() + theme(strip.text.y = element_text(angle = 180, 
    size = 5), strip.background = element_blank(), 
    legend.position = c(0.99, 0.01), legend.justification = c(1, 
        0), axis.text.y = element_blank(), axis.ticks.y = element_blank()) + 
    xlab(&amp;quot;reference sequence&amp;quot;) + ylab(&amp;quot;assembly&amp;quot;) + 
    scale_colour_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-19-MumplotsWithGgplot2_files/figure-html/diag-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../imgs/mumplot-example.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;percent-identity-and-coverage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Percent identity and coverage&lt;/h2&gt;
&lt;p&gt;Another useful MUMmerplot represents the position of each aligned segment and its percent similarity.&lt;/p&gt;
&lt;p&gt;This graph could be useful to decide which size/similarity threshold to use when filtering low alignments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mumgp %&amp;lt;&amp;gt;% mutate(similarity = 1 - error/abs(qe - qs))
mumgp.filt %&amp;lt;&amp;gt;% mutate(similarity = 1 - error/abs(qe - 
    qs))

ggplot(mumgp, aes(x = rs, xend = re, y = similarity, 
    yend = similarity)) + geom_segment() + theme_bw() + 
    xlab(&amp;quot;reference sequence&amp;quot;) + ylab(&amp;quot;similarity&amp;quot;) + 
    ggtitle(&amp;quot;All contigs&amp;quot;) + ylim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-19-MumplotsWithGgplot2_files/figure-html/simgraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mumgp.filt, aes(x = rs, xend = re, y = similarity, 
    yend = similarity)) + geom_segment() + theme_bw() + 
    xlab(&amp;quot;reference sequence&amp;quot;) + ylab(&amp;quot;similarity&amp;quot;) + 
    ggtitle(&amp;quot;At least 10 Kbp aligned&amp;quot;) + ylim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-19-MumplotsWithGgplot2_files/figure-html/simgraph-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To better highlighted which region in the reference is covered, I annotate each base of the reference with the maximum similarity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;maxSimilarityDisjoin &amp;lt;- function(df) {
    ref.ir = GRanges(&amp;quot;X&amp;quot;, IRanges(df$rs, df$re), similarity = df$similarity)
    ## Efficient clean up of low similarity within high
    ## similarity
    step = 1
    while (step &amp;gt; 0) {
        largealign = ref.ir[head(order(rank(-ref.ir$similarity), 
            rank(-width(ref.ir))), step * 1000)]
        ol = findOverlaps(ref.ir, largealign, type = &amp;quot;within&amp;quot;) %&amp;gt;% 
            as.data.frame %&amp;gt;% mutate(simW = ref.ir$similarity[queryHits], 
            simL = largealign$similarity[subjectHits]) %&amp;gt;% 
            filter(simW &amp;lt; simL)
        if (length(largealign) == length(ref.ir)) {
            step = 0
        } else {
            step = step + 1
        }
        ref.ir = ref.ir[-ol$queryHits]
    }
    ## Disjoin and annotate with the max similarity
    ref.dj = disjoin(c(ref.ir, GRanges(&amp;quot;X&amp;quot;, IRanges(min(df$rs), 
        max(df$rs)), similarity = 0)))
    ol = findOverlaps(ref.ir, ref.dj) %&amp;gt;% as.data.frame %&amp;gt;% 
        mutate(similarity = ref.ir$similarity[queryHits]) %&amp;gt;% 
        group_by(subjectHits) %&amp;gt;% summarize(similarity = max(similarity))
    ref.dj$similarity = 0
    ref.dj$similarity[ol$subjectHits] = ol$similarity
    as.data.frame(ref.dj)
}

mumgp.sim = maxSimilarityDisjoin(mumgp)

mumgp.sim %&amp;gt;% select(similarity, start, end) %&amp;gt;% gather(end, 
    pos, 2:3) %&amp;gt;% ggplot() + geom_line(aes(x = pos, 
    y = similarity), alpha = 0.5, color = &amp;quot;red&amp;quot;) + 
    theme_bw() + xlab(&amp;quot;reference sequence&amp;quot;) + ylab(&amp;quot;similarity&amp;quot;) + 
    ggtitle(&amp;quot;All contigs&amp;quot;) + ylim(0, 1) + geom_segment(aes(x = rs, 
    xend = re, y = similarity, yend = similarity), 
    data = mumgp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-19-MumplotsWithGgplot2_files/figure-html/simcov-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mumgp.sim) + geom_segment(aes(x = start, xend = end, 
    yend = similarity, y = similarity), color = &amp;quot;red&amp;quot;, 
    size = 2) + theme_bw() + xlab(&amp;quot;reference sequence&amp;quot;) + 
    ylab(&amp;quot;similarity&amp;quot;) + ylim(0, 1) + geom_segment(aes(x = rs, 
    xend = re, y = similarity, yend = similarity), 
    data = mumgp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-19-MumplotsWithGgplot2_files/figure-html/simcov-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this graph we could compare different assemblies or before/after filtering:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mumgp.filt.sim = maxSimilarityDisjoin(mumgp.filt)

mumgp.filt.m = rbind(mumgp.sim %&amp;gt;% mutate(filter = &amp;quot;before&amp;quot;), 
    mumgp.filt.sim %&amp;gt;% mutate(filter = &amp;quot;after&amp;quot;))

mumgp.filt.m %&amp;gt;% select(similarity, start, end, filter) %&amp;gt;% 
    gather(end, pos, 2:3) %&amp;gt;% ggplot(aes(x = pos, y = similarity, 
    colour = filter)) + geom_line(alpha = 0.8) + theme_bw() + 
    xlab(&amp;quot;reference sequence&amp;quot;) + ylab(&amp;quot;similarity&amp;quot;) + 
    ylim(0, 1) + scale_colour_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-19-MumplotsWithGgplot2_files/figure-html/simcovcomp-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not so pretty but we see that a few region are not covered any more after our filtering. Maybe something like this instead :&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mumgp.filt.m %&amp;gt;% filter(similarity == 0) %&amp;gt;% ggplot(aes(x = start, 
    xend = end, y = filter, yend = filter)) + geom_segment(size = 10) + 
    theme_bw() + xlab(&amp;quot;reference sequence&amp;quot;) + ylab(&amp;quot;filter&amp;quot;) + 
    scale_colour_brewer(palette = &amp;quot;Set1&amp;quot;) + ggtitle(&amp;quot;Reference regions not covered&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-19-MumplotsWithGgplot2_files/figure-html/simcovcomptrack-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Regression sandbox</title>
          <link>/2017/09/16/regression-sandbox/</link>
          <pubDate>Sat, 16 Sep 2017 00:00:00 UTC</pubDate>
          <author></author>
          <guid>/2017/09/16/regression-sandbox/</guid>
          <description>&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(broom)
library(magrittr)
library(dplyr)
library(knitr)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic regression&lt;/h2&gt;
&lt;div id=&#34;one-way-or-another&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://youtu.be/4kg9LasvLFE&#34;&gt;One way or another&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;If we have two binary variables and we want to see if they are associated we could use a logistic regression. How do we decide which variable to be the predictor and which variable to observed variable ?&lt;/p&gt;
&lt;p&gt;In theory there shouldn’t be any differences but let’s check with a dummy example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df = data.frame(x = sample(c(FALSE, TRUE), 100, TRUE))
df$y = df$x
df$y[1:70] = sample(c(FALSE, TRUE), 70, TRUE)

glm(y ~ x, data = df, family = binomial()) %&amp;gt;% tidy %&amp;gt;% 
    kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2336149&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3070802&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.7607617&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4467994&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;xTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1745982&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4256623&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7594604&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0057897&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(x ~ y, data = df, family = binomial()) %&amp;gt;% tidy %&amp;gt;% 
    kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4054651&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3227486&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.256288&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2090117&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;yTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1745982&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4256624&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.759460&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0057897&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$z = runif(100)
glm(y ~ x + z, data = df, family = binomial()) %&amp;gt;% 
    tidy %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.5930811&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5436028&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.0910191&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2752645&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;xTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2012521&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4293265&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7979921&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0051421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;z&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6601875&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8199692&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8051369&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4207407&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(x ~ y + z, data = df, family = binomial()) %&amp;gt;% 
    tidy %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1246648&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5141828&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2424523&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8084297&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;yTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1996170&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4291206&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7955243&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0051816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;z&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.5586854&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8023449&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.6963157&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4862311&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Adding another predictor doesn’t change the estimates either.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;Just to make sure I understand the estimates correctly. It represents the log odds ratio change for each “unit” of the predictor. In the case of a binary variable, the log odds ratio between the two groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(y ~ x, data = df, family = binomial()) %&amp;gt;% tidy %&amp;gt;% 
    kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2336149&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3070802&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.7607617&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4467994&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;xTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1745982&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4256623&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7594604&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0057897&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;odds.y.ifx = mean(subset(df, x)$y)/mean(!subset(df, 
    x)$y)
odds.y.ifnotx = mean(subset(df, !x)$y)/mean(!subset(df, 
    !x)$y)
log(odds.y.ifx/odds.y.ifnotx)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.174598&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;extreme-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extreme cases&lt;/h3&gt;
&lt;p&gt;How efficient is the logistic regression when there is an imbalance between different types of observations ? For example if just a few genomic regions overlap an interesting annotation and I want to test is the overlap is significant.&lt;/p&gt;
&lt;p&gt;Let’s look at the worst cases when there are only 1 observation for a particular class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df = data.frame(y = sample(c(FALSE, TRUE), 100, TRUE))
df$x = 1:nrow(df) %in% sample.int(nrow(df), 1)
glm(y ~ x, data = df, family = binomial()) %&amp;gt;% tidy %&amp;gt;% 
    kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1010961&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2012644&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5023050&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6154530&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;xTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.4649721&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1455.3975462&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0106259&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9915219&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Although the significance is low, the estimate seems quite high. I’ll repeat this process a bunch of time and with different number of supporting observations to have an idea of the distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ext.df = lapply(1:500, function(ii) {
    res = lapply(1:10, function(ssi) {
        df$x = 1:nrow(df) %in% sample.int(nrow(df), 
            ssi)
        glm(y ~ x, data = df, family = binomial()) %&amp;gt;% 
            tidy %&amp;gt;% mutate(rep = ii, ss = ssi)
    })
    do.call(rbind, res)
})
ext.df = do.call(rbind, ext.df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ext.df %&amp;gt;% filter(term == &amp;quot;xTRUE&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate)) + 
    geom_density(fill = &amp;quot;grey50&amp;quot;) + facet_grid(ss ~ 
    ., scales = &amp;quot;free&amp;quot;) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-16-Regression_files/figure-html/lrextsimgraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems like the estimate “inflation” is problematic mostly when there are only 1 or 2 supporting observations. If there are more than 5 supporting observations the estimate is correctly centered in 0.&lt;/p&gt;
&lt;p&gt;This problem is in fact called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Separation_(statistics)&#34;&gt;problem of separation&lt;/a&gt;. There are two approaches to deal with it:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Firth logistic regression.&lt;/li&gt;
&lt;li&gt;Exact logistic regression.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;a href=&#34;https://cran.r-project.org/web/packages/rms/index.html&#34;&gt;&lt;code&gt;rms&lt;/code&gt; package&lt;/a&gt; from &lt;a href=&#34;http://www.fharrell.com/2017/01/introduction.html&#34;&gt;Frank Harell&lt;/a&gt;. It implements a penalized maximum likelihood estimation of the model coefficients through the &lt;code&gt;lrm&lt;/code&gt; function which has a &lt;code&gt;penalty=&lt;/code&gt; parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rms)
extrms.df = lapply(1:200, function(ii) {
    res = lapply(1:10, function(ssi) {
        res = lapply(c(1, 3, 5), function(pen) {
            df$x = 1:nrow(df) %in% sample.int(nrow(df), 
                ssi)
            cc = lrm(y ~ x, data = df, penalty = pen)$coefficient
            data.frame(term = names(cc), estimate = cc, 
                rep = ii, ss = ssi, penalty = pen, 
                stringsAsFactors = FALSE)
        })
        do.call(rbind, res)
    })
    do.call(rbind, res)
})
extrms.df = do.call(rbind, extrms.df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extrms.df %&amp;gt;% filter(term == &amp;quot;x&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate)) + 
    geom_density(fill = &amp;quot;grey50&amp;quot;) + facet_grid(ss ~ 
    penalty, scales = &amp;quot;free&amp;quot;) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-16-Regression_files/figure-html/lrmsgraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It definitely helps: the estimates are now much closer to 0. I don’t see much difference between penalties 1, 3 or 5.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://cran.r-project.org/web/packages/logistf/index.html&#34;&gt;&lt;code&gt;logistf&lt;/code&gt; package&lt;/a&gt;. It implements Firth’s bias reduction method with its &lt;code&gt;logistf&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(logistf)
extstf.df = lapply(1:200, function(ii) {
    res = lapply(1:10, function(ssi) {
        df$x = 1:nrow(df) %in% sample.int(nrow(df), 
            ssi)
        cc = logistf(y ~ x, data = df)$coefficient
        data.frame(term = names(cc), estimate = cc, 
            rep = ii, ss = ssi, stringsAsFactors = FALSE)
    })
    do.call(rbind, res)
})
extstf.df = do.call(rbind, extstf.df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extstf.df %&amp;gt;% filter(term == &amp;quot;xTRUE&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate)) + 
    geom_density(fill = &amp;quot;grey50&amp;quot;) + facet_grid(ss ~ 
    ., scales = &amp;quot;free&amp;quot;) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-16-Regression_files/figure-html/lrreggraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This works well too.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;more-advanced-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More advanced models&lt;/h2&gt;
&lt;p&gt;A dummy example with some code for Generalized Additive Models, LOESS and SVM.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nb.samp = 1000
df = data.frame(x = runif(nb.samp, 0, 100))
df$y = rnorm(nb.samp, 0, 5) + abs(df$x - 25)
df$y = ifelse(df$x &amp;gt; 40, rnorm(nb.samp, 0, 5) - df$x * 
    df$x/300 + 20, df$y)
ggplot(df, aes(x = x, y = y)) + geom_point(alpha = 0.5) + 
    theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-16-Regression_files/figure-html/blm-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm.o = glm(y ~ x, data = df)
loess.o = loess(y ~ x, data = df)
library(mgcv)
gam.o = gam(y ~ s(x, bs = &amp;quot;cs&amp;quot;), data = df)
library(e1071)
svm.o = svm(y ~ x, data = df)

pred.df = rbind(df %&amp;gt;% mutate(y = predict(glm.o), model = &amp;quot;glm&amp;quot;), 
    df %&amp;gt;% mutate(y = predict(gam.o), model = &amp;quot;gam&amp;quot;), 
    df %&amp;gt;% mutate(y = predict(loess.o), model = &amp;quot;LOESS&amp;quot;), 
    df %&amp;gt;% mutate(y = predict(svm.o), model = &amp;quot;SVM&amp;quot;))

ggplot(df, aes(x = x, y = y)) + geom_point(alpha = 0.2) + 
    geom_line(aes(colour = model), size = 2, alpha = 0.9, 
        data = pred.df) + theme_bw() + scale_colour_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-16-Regression_files/figure-html/blmmodels-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Word Cloud in R</title>
          <link>/2016/02/26/word-cloud-in-r/</link>
          <pubDate>Fri, 26 Feb 2016 00:00:00 UTC</pubDate>
          <author></author>
          <guid>/2016/02/26/word-cloud-in-r/</guid>
          <description>&lt;p&gt;The &lt;a href=&#34;https://cran.r-project.org/web/packages/wordcloud/index.html&#34;&gt;&lt;code&gt;wordcloud&lt;/code&gt; package&lt;/a&gt; is available on CRAN.&lt;/p&gt;
&lt;div id=&#34;fake-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fake words&lt;/h2&gt;
&lt;p&gt;I create fake words to see a bit how the command is working.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wordcloud)
createWords &amp;lt;- function(w.l = 3) paste(sample(letters, 
    w.l, TRUE), collapse = &amp;quot;&amp;quot;)
words = sapply(1:200, function(e) createWords(runif(1, 
    3, 10)))
freq = c(sample(1:30, 190, T), sample(30:150, 10, T))
freq = freq/sum(freq)
wordcloud(words, freq)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2016-02-26-wordcloud_files/figure-html/example-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Big words in the center
wordcloud(words, freq, random.order = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2016-02-26-wordcloud_files/figure-html/example-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Max word number
wordcloud(words, freq, max.words = 50)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2016-02-26-wordcloud_files/figure-html/example-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Rotation: proportion of 90 degree
wordcloud(words, freq, rot.per = 0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2016-02-26-wordcloud_files/figure-html/example-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Color the most frequent words
wordcloud(words, freq, colors = c(&amp;quot;black&amp;quot;, &amp;quot;blue&amp;quot;, 
    &amp;quot;red&amp;quot;), random.order = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2016-02-26-wordcloud_files/figure-html/example-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Color for each word
wordcloud(words, freq, colors = sample(c(&amp;quot;black&amp;quot;, &amp;quot;blue&amp;quot;, 
    &amp;quot;red&amp;quot;), length(words), TRUE), random.order = FALSE, 
    ordered.colors = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2016-02-26-wordcloud_files/figure-html/example-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;command-history&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Command history&lt;/h2&gt;
&lt;p&gt;I retrieved the commands from my &lt;code&gt;.bash_history&lt;/code&gt; files (laptop and HPCs) and I want to make a word cloud showing the commands I use the most.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(RColorBrewer)
cmds = read.table(&amp;quot;../../data/bash-commands.tsv.gz&amp;quot;, 
    as.is = TRUE)
colnames(cmds) = c(&amp;quot;cmd&amp;quot;, &amp;quot;machine&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;laptop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Laptop&lt;/h3&gt;
&lt;p&gt;By default the maximum history size was set to 500 commands so I don’t have the full set of commands, just the last 500. (I increased the limit, see you in 10,000 commands.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cmds.s = cmds %&amp;gt;% filter(machine == &amp;quot;laptop&amp;quot;, !grepl(&amp;quot;=&amp;quot;, 
    cmd), !grepl(&amp;quot;\\.&amp;quot;, cmd), !grepl(&amp;quot;/&amp;quot;, cmd)) %&amp;gt;% 
    group_by(cmd) %&amp;gt;% summarize(n = n()) %&amp;gt;% mutate(freq = n/sum(n))
wordcloud(cmds.s$cmd, cmds.s$freq, colors = c(&amp;quot;black&amp;quot;, 
    brewer.pal(8, &amp;quot;Set1&amp;quot;)), random.order = FALSE, scale = c(10, 
    1), title = &amp;quot;All&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2016-02-26-wordcloud_files/figure-html/cmdlt-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hpc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;HPC&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cmds.s = cmds %&amp;gt;% filter(machine != &amp;quot;laptop&amp;quot;, !grepl(&amp;quot;=&amp;quot;, 
    cmd), !grepl(&amp;quot;\\.&amp;quot;, cmd), !grepl(&amp;quot;/&amp;quot;, cmd), !grepl(&amp;quot;\\$&amp;quot;, 
    cmd), !grepl(&amp;quot;\\:&amp;quot;, cmd)) %&amp;gt;% group_by(cmd) %&amp;gt;% 
    summarize(n = n()) %&amp;gt;% mutate(freq = n/sum(n))
wordcloud(cmds.s$cmd, cmds.s$freq, colors = c(&amp;quot;black&amp;quot;, 
    brewer.pal(8, &amp;quot;Set1&amp;quot;)), random.order = FALSE, scale = c(10, 
    1), title = &amp;quot;All&amp;quot;, min.freq = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2016-02-26-wordcloud_files/figure-html/cmdhpc-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Good see the usual suspects &lt;code&gt;ls&lt;/code&gt; and &lt;code&gt;cd&lt;/code&gt; and their “typo” versions &lt;code&gt;;s&lt;/code&gt;/&lt;code&gt;ks&lt;/code&gt;/&lt;code&gt;ld&lt;/code&gt; and &lt;code&gt;xs&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
        </item>
      
    

  </channel>
</rss>

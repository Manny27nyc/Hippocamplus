<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hippocamplus </title>
    <link>/Hippocamplus/tags/stats/</link>
    <language>en-us</language>
    <author></author>
    <rights>(C) 2018</rights>
    <updated>2018-06-09 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Clustering into same size clusters</title>
          <link>/Hippocamplus/2018/06/09/cluster-same-size/</link>
          <pubDate>Sat, 09 Jun 2018 00:00:00 UTC</pubDate>
          <author></author>
          <guid>/Hippocamplus/2018/06/09/cluster-same-size/</guid>
          <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#iterative-dichotomy&#34;&gt;Iterative dichotomy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iterative-nearest-neighbor&#34;&gt;Iterative nearest neighbor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#same-size-k-means-variation&#34;&gt;Same-size k-Means Variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#test-data&#34;&gt;Test data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#test-data-1&#34;&gt;Test data 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#test-data-2&#34;&gt;Test data 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extra-optimization&#34;&gt;Extra: optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34;&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;I would like to cluster points into groups of similar size. For example I would like to group 1000 points into clusters of around 10 points each. The two aspects that are important here are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The cluster size distribution, or the deviation from the desired cluster size.&lt;/li&gt;
&lt;li&gt;The quality of the clusters, i.e. how similar are points within a cluster.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In addition to the typical hierarchical clustering approach, I will test the following iterative approaches:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Iterative dichotomy: large clusters are split in two until around the desired size (using hierarchical clustering).&lt;/li&gt;
&lt;li&gt;Iterative nearest neighbor: a point and its closest neighboring points are assigned to a cluster and removed before processing another point.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://elki-project.github.io/tutorial/same-size_k_means&#34;&gt;Same-size k-Means Variation&lt;/a&gt; that some quick googling returned.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a baseline, points will be randomly clustered into same-size clusters.&lt;/p&gt;
&lt;p&gt;In the following &lt;span class=&#34;math&#34;&gt;\(s\)&lt;/span&gt; is the target cluster size.&lt;/p&gt;
&lt;div id=&#34;iterative-dichotomy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Iterative dichotomy&lt;/h2&gt;
&lt;p&gt;Starting with one cluster containing all the points, a cluster is split in two if larger that &lt;span class=&#34;math&#34;&gt;\(1.5*s\)&lt;/span&gt;. When all clusters are smaller than &lt;span class=&#34;math&#34;&gt;\(1.5*s\)&lt;/span&gt;, the process stops.&lt;/p&gt;
&lt;p&gt;The points are split in two using hierarchical clustering. I will try different linkage criteria. My guess is that the Ward criterion will be good at this because it tends to produce balanced dendrograms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iterative-nearest-neighbor&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Iterative nearest neighbor&lt;/h2&gt;
&lt;p&gt;While there are more than &lt;span class=&#34;math&#34;&gt;\(s\)&lt;/span&gt; unassigned points:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A point is selected. Randomly or following a rule (see below).&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math&#34;&gt;\(s-1\)&lt;/span&gt; closest points are found and assigned to a new cluster.&lt;/li&gt;
&lt;li&gt;These points are removed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the total number of points is not a multiple of &lt;span class=&#34;math&#34;&gt;\(s\)&lt;/span&gt;, the remaining points could be either assigned to their own clusters or to an existing cluster. Actually, we completely control the cluster sizes here so we could fix the size of some clusters to &lt;span class=&#34;math&#34;&gt;\(s+1\)&lt;/span&gt; beforehand to avoid leftovers and ensure balanced sizes.&lt;/p&gt;
&lt;p&gt;In the first step, a point is selected. I’ll start by choosing a point randomly (out of the unassigned points). Eventually I could try picking the points with close neighbors, or the opposite, far from other points. I’ll use the mean distance between a point and the others to define the order at which points are processed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;same-size-k-means-variation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Same-size k-Means Variation&lt;/h2&gt;
&lt;p&gt;As explained in a few pages online (e.g. &lt;a href=&#34;https://elki-project.github.io/tutorial/same-size_k_means&#34;&gt;here&lt;/a&gt;), one approach consists of using K-means to derive centers and then assigning the same amount of points to each center/cluster.&lt;/p&gt;
&lt;p&gt;In the proposed approach the points are ordered by their distance to the closest center minus the distance to the farthest cluster. Each point is assigned to the best cluster in this order. If the best cluster is full, the second best is chosen, etc.&lt;/p&gt;
&lt;p&gt;I’ll also try to order the points by the distance to the closest center, by the distance to the farthest cluster, or using a random order.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;test-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Test data&lt;/h2&gt;
&lt;p&gt;I’ll test the different approaches on dummy data with Gaussian distributions and some outliers. Both test datasets contains, 1000 points in two dimensions and includes 100 outliers. The second test dataset contains two groups, one larger than the other.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;Let’s aim at clusters of around &lt;span class=&#34;math&#34;&gt;\(s=21\)&lt;/span&gt; points. Not 20 because that way there will be left-over points (more realistic).&lt;/p&gt;
&lt;div id=&#34;test-data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Test data 1&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-4-4.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;test-data-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Test data 2&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-5-3.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-5-4.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;iterative dichotomy&lt;/em&gt; approach is not as bad as I thought, especially using Ward linkage criterion, but it doesn’t really controls for the final cluster size. We end up with most clusters around the desired size but the size of some clusters still vary by a factor of 2 or more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The nearest neighbor approach (&lt;em&gt;maxD&lt;/em&gt; variant), is the best approach in my opinion.&lt;/strong&gt; The cluster size is completely controlled and the mean/maximum pairwise distance for points in the same cluster is similar (or better) to other approaches.&lt;/p&gt;
&lt;p&gt;The K-means approach didn’t perform as well but we can keep it in mind if the number of points is very large, as it is much more memory efficient (no need for a pairwise distance matrix).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extra-optimization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extra: optimization&lt;/h2&gt;
&lt;p&gt;The nearest neighbor approach requires a &lt;code&gt;while&lt;/code&gt; loop, which is not efficient in R. Maybe implementing the loop with Rcpp could speed up the computation (in practice I would like to run this on up to 10K points).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Three times faster with Rcpp!&lt;/p&gt;
&lt;p&gt;Safety check, are the results actually the same ?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2018-06-09-ClusterEqualSize_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yes. Ouf…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The source code of this page can be found &lt;a href=&#34;https://github.com/jmonlong/Hippocamplus/tree/master/content/post/2018-06-09-ClusterEqualSize.Rmd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>tSNE and clustering</title>
          <link>/Hippocamplus/2018/02/13/tsne-and-clustering/</link>
          <pubDate>Tue, 13 Feb 2018 00:00:00 UTC</pubDate>
          <author></author>
          <guid>/Hippocamplus/2018/02/13/tsne-and-clustering/</guid>
          <description>&lt;p&gt;tSNE can give really nice results when we want to visualize many groups of multi-dimensional points. Once the 2D graph is done we might want to identify which points cluster in the tSNE blobs.&lt;/p&gt;
&lt;p&gt;Using simulated and real data, I’ll try different methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hierarchical clustering&lt;/li&gt;
&lt;li&gt;K-means&lt;/li&gt;
&lt;li&gt;Gaussian mixture&lt;/li&gt;
&lt;li&gt;Density-based clustering&lt;/li&gt;
&lt;li&gt;Louvain community detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; If &amp;lt;30K points, hierarchical clustering is robust, easy to use and with reasonable computing time. KNN + Louvain is fast and works well in general.&lt;/p&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr)
library(magrittr)
library(ggrepel)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;div id=&#34;normally-distributed-points&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Normally distributed points&lt;/h3&gt;
&lt;p&gt;First, I’ll simulate an easy situation with 10 different groups. 5,000 points are distributed following Gaussian distributions in 100 dimensions. Points are randomly assigned a group. For each group, 3 dimensions are randomly selected and the points shifted.&lt;/p&gt;
&lt;p&gt;Because there are 10 groups that differ in different dimensions, a PCA shouldn’t be able to separate all the groups with the first two components. That’s when the tSNE comes in to do its magic (easily).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
N = 5000
D = 100
data.norm = matrix(rnorm(N * D, 2), N)
groups.probs = runif(10)
groups = sample(1:10, N, TRUE, groups.probs/sum(groups.probs))
for (gp in unique(groups)) {
    dev = rep(1, D)
    dev[sample.int(D, 3)] = runif(3, -10, 10)
    data.norm[which(groups == gp), ] = data.norm[which(groups == 
        gp), ] %*% diag(dev)
}
info.norm = tibble(truth = factor(groups))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The PCA and tSNE look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca.norm = prcomp(data.norm)
info.norm %&amp;lt;&amp;gt;% cbind(pca.norm$x[, 1:4])
ggplot(info.norm, aes(x = PC1, y = PC2, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/pcatsne-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(info.norm, aes(x = PC3, y = PC4, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/pcatsne-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see something but it’s not so clear, let’s run the tSNE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rtsne)
tsne.norm = Rtsne(pca.norm$x, pca = FALSE)
info.norm %&amp;lt;&amp;gt;% mutate(tsne1 = tsne.norm$Y[, 1], tsne2 = tsne.norm$Y[, 
    2])
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = truth)) + 
    geom_point(alpha = 0.3) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/tsne-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 83.76 s&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;real-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Real data&lt;/h3&gt;
&lt;p&gt;As a real-life example, I use the data that motivated this exploration. It contains a bit more than 26K points and the tSNE looks like that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tsne.real = read.csv(&amp;quot;https://docs.google.com/uc?id=1KArwfOd5smzuCsrpgW9Xpf9I06VOW4ga&amp;amp;export=download&amp;quot;)
info.real = tsne.real
ggplot(tsne.real, aes(x = tsne1, y = tsne2)) + geom_point(alpha = 0.1) + 
    theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/real-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Once built, it’s fast to try different number clusters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Different linkage criteria to match the behavior we want.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Doesn’t scale well. High memory usage and computation time when &amp;gt;30K.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc.norm = hclust(dist(tsne.norm$Y))
info.norm$hclust = factor(cutree(hc.norm, 9))
hc.norm.cent = info.norm %&amp;gt;% group_by(hclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.norm.cent) + guides(colour = FALSE) + 
    ggtitle(&amp;quot;Linkage criterion: Complete&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/hcnorm-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc.norm = hclust(dist(tsne.norm$Y), method = &amp;quot;ward.D&amp;quot;)
info.norm$hclust = factor(cutree(hc.norm, 9))
hc.norm.cent = info.norm %&amp;gt;% group_by(hclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.norm.cent) + guides(colour = FALSE) + 
    ggtitle(&amp;quot;Linkage criterion: Ward&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/hcnorm-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now on real data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc.real = hclust(dist(tsne.real))
info.real$hclust = factor(cutree(hc.real, 18))
hc.real.cent = info.real %&amp;gt;% group_by(hclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.real.cent) + guides(colour = FALSE) + 
    ggtitle(&amp;quot;Linkage criterion: Complete&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/hcreal-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc.real = hclust(dist(tsne.real), method = &amp;quot;ward.D&amp;quot;)
info.real$hclust = factor(cutree(hc.real, 18))
hc.real.cent = info.real %&amp;gt;% group_by(hclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = hclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = hclust), 
    data = hc.real.cent) + guides(colour = FALSE) + 
    ggtitle(&amp;quot;Linkage criterion: Ward&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/hcreal2-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 38.01 s&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For both data, Ward gives the best clusters. For example it splits the top-left clusters better in the real data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kmeans&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Kmeans&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Very fast.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Simple.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km.norm = kmeans(tsne.norm$Y, 9, nstart = 100)
info.norm$kmeans = factor(km.norm$cluster)
km.cent = info.norm %&amp;gt;% group_by(kmeans) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE) + ggtitle(&amp;quot;9 clusters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/kmnorm-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km.norm = kmeans(tsne.norm$Y, 10, nstart = 100)
info.norm$kmeans = factor(km.norm$cluster)
km.cent = info.norm %&amp;gt;% group_by(kmeans) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE) + ggtitle(&amp;quot;10 clusters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/kmnorm-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because it’s not working well for cluster that are not “round”, we need to ask for more clusters. In practice we’ll need to merge back together the clusters that were fragmented.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
km.real = kmeans(tsne.real, 24, nstart = 200, iter.max = 100)
info.real$kmeans = factor(km.real$cluster)
km.cent = info.real %&amp;gt;% group_by(kmeans) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/kmreal-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not perfect in the middle-left big cluster: cluster 11 is grabbing points from the bottom blob. Maybe increasing the number of clusters could fix this?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
km.real = kmeans(tsne.real, 25, nstart = 200, iter.max = 100)
info.real$kmeans = factor(km.real$cluster)
km.cent = info.real %&amp;gt;% group_by(kmeans) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = kmeans)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = kmeans), 
    data = km.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/kmrealmore-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 15.48 s&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Better. Same as with the other methods: we need to manually tweak the parameters to obtain the clustering we want…&lt;/p&gt;
&lt;p&gt;Note: using several starting points help getting more robust results (&lt;code&gt;nstart=&lt;/code&gt;). Increasing the number of iterations helps too (&lt;code&gt;iter.max=&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mclust&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mclust&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Better clusters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Can find the best K (number of clusters (although slowly).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Slow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Need to be recomputed for each choice of K (number of clusters).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mclust)
mc.norm = Mclust(tsne.norm$Y, 9)
info.norm$mclust = factor(mc.norm$classification)
mc.cent = info.norm %&amp;gt;% group_by(mclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/mcnorm-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even the elongated cluster is nicely identified and we don’t need to split it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
mc.real = Mclust(tsne.real, 20, initialization = list(subset = sample.int(nrow(tsne.real), 
    1000)))
info.real$mclust = factor(mc.real$classification)
mc.cent = info.real %&amp;gt;% group_by(mclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/mcreal-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sometimes the results are a bit surprising. For example, points are assigned to cluster far away or there is another cluster in between (e.g. clusters 6 and 17). As usual changing the number of clusters helps.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
mc.real = Mclust(tsne.real, 24, initialization = list(subset = sample.int(nrow(tsne.real), 
    1000)))
info.real$mclust = factor(mc.real$classification)
mc.cent = info.real %&amp;gt;% group_by(mclust) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = mclust)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = mclust), 
    data = mc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/mcreal2-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 90.61 s&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Note: I had to use the sub-sampling trick to speed up the process, otherwise it was taking too long. Using &lt;code&gt;initialization=list(subset=sample.int(nrow(tsne.real), 1000))&lt;/code&gt;, only a thousand points are used for the EM (but all the points are assigned to a cluster at the end).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-based-clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Density-based clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;+&lt;/strong&gt; Can find clusters with different “shapes”.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Bad on real/noisy data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-&lt;/strong&gt; Slow when many points.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fpc)
ds.norm = dbscan(tsne.norm$Y, 2)
info.norm$density = factor(ds.norm$cluster)
ds.cent = info.norm %&amp;gt;% group_by(density) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = density)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = density), 
    data = ds.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/densnorm-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Woah, it found the small cluster !&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ds.real = dbscan(tsne.real, 1)
info.real$density = factor(ds.real$cluster)
ds.cent = info.real %&amp;gt;% group_by(density) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = density)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = density), 
    data = ds.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/densreal-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 100.98 s&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ouch…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;knn-graph-and-louvain-community-detection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;KNN graph and Louvain community detection&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(igraph)
library(FNN)
k = 100
knn.norm = get.knn(as.matrix(tsne.norm$Y), k = k)
knn.norm = data.frame(from = rep(1:nrow(knn.norm$nn.index), 
    k), to = as.vector(knn.norm$nn.index), weight = 1/(1 + 
    as.vector(knn.norm$nn.dist)))
nw.norm = graph_from_data_frame(knn.norm, directed = FALSE)
nw.norm = simplify(nw.norm)
lc.norm = cluster_louvain(nw.norm)
info.norm$louvain = as.factor(membership(lc.norm))
lc.cent = info.norm %&amp;gt;% group_by(louvain) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = louvain)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = louvain), 
    data = lc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/louvnorm-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Playing with the resolution parameter we can get more/less communities. For &lt;code&gt;gamma=.3&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lc.norm = cluster_louvain(nw.norm, gamma = 0.3)
info.norm$louvain = as.factor(membership(lc.norm))
lc.cent = info.norm %&amp;gt;% group_by(louvain) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.norm, aes(x = tsne1, y = tsne2, colour = louvain)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = louvain), 
    data = lc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/gamma-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On real data and using &lt;code&gt;gamma=.1&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k = 100
knn.real = get.knn(as.matrix(tsne.real), k = k)
knn.real = data.frame(from = rep(1:nrow(knn.real$nn.index), 
    k), to = as.vector(knn.real$nn.index), weight = 1/(1 + 
    as.vector(knn.real$nn.dist)))
nw.real = graph_from_data_frame(knn.real, directed = FALSE)
nw.real = simplify(nw.real)
lc.real = cluster_louvain(nw.real, gamma = 0.1)
info.real$louvain = as.factor(membership(lc.real))
lc.cent = info.real %&amp;gt;% group_by(louvain) %&amp;gt;% select(tsne1, 
    tsne2) %&amp;gt;% summarize_all(mean)
ggplot(info.real, aes(x = tsne1, y = tsne2, colour = louvain)) + 
    geom_point(alpha = 0.3) + theme_bw() + geom_label_repel(aes(label = louvain), 
    data = lc.cent) + guides(colour = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-12-01-tSNEclustering_files/figure-html/louvreal-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time for this code chunk: 18.86 s&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Pretty good.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PS:&lt;/em&gt; I added the resolution parameter &lt;code&gt;gamma&lt;/code&gt; in the &lt;em&gt;igraph&lt;/em&gt; function for the Louvain clustering. While it was easy to change in the C code, compiling &lt;em&gt;igraph&lt;/em&gt; from source was a pain. I couldn’t get it to work on OSX but I managed to install this modified version of igraph on Linux (see &lt;a href=&#34;https://github.com/jmonlong/Hippocamplus/tree/config/R/rigraph_gammalouvain&#34;&gt;instructions&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;If not too many points or too many groups, &lt;strong&gt;hierarchical clustering&lt;/strong&gt; might be enough. Especially with the Ward criterion, it worked well for both simulated and real data. Once the hierarchy is built, it’s fast to try different values for the number of clusters. Also, in the real data, I could get satisfactory results using a lower number of clusters than for the K-means (18 vs 25).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If there are too many points&lt;/strong&gt; (e.g. &amp;gt;30K), hierarchical clustering might be too demanding and I would fall back to &lt;strong&gt;KNN+Louvain&lt;/strong&gt;. It’s fast enough and the results are pretty good.&lt;/p&gt;
&lt;p&gt;The more advanced methods are good to keep in mind if the points ever form diverse or unusual shapes.&lt;/p&gt;
&lt;p&gt;I learned two &lt;strong&gt;tricks to improve the performance&lt;/strong&gt; of the methods: increasing the number of iterations and starting points for the K-means, and sub-sampling for the EM clustering.&lt;/p&gt;
&lt;p&gt;Clustering points from the tSNE is good to explore the groups that we visually see in the tSNE but &lt;strong&gt;if we want more meaningful clusters&lt;/strong&gt; we could run these methods &lt;strong&gt;in the PC space directly&lt;/strong&gt;. The KNN + Louvain community clustering, for example, is used in single cell sequencing analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Regression sandbox</title>
          <link>/Hippocamplus/2017/09/16/regression-sandbox/</link>
          <pubDate>Sat, 16 Sep 2017 00:00:00 UTC</pubDate>
          <author></author>
          <guid>/Hippocamplus/2017/09/16/regression-sandbox/</guid>
          <description>&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(broom)
library(magrittr)
library(dplyr)
library(knitr)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic regression&lt;/h2&gt;
&lt;div id=&#34;one-way-or-another&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://youtu.be/4kg9LasvLFE&#34;&gt;One way or another&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;If we have two binary variables and we want to see if they are associated we could use a logistic regression. How do we decide which variable to be the predictor and which variable to observed variable ?&lt;/p&gt;
&lt;p&gt;In theory there shouldn’t be any differences but let’s check with a dummy example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df = data.frame(x = sample(c(FALSE, TRUE), 100, TRUE))
df$y = df$x
df$y[1:70] = sample(c(FALSE, TRUE), 70, TRUE)

glm(y ~ x, data = df, family = binomial()) %&amp;gt;% tidy %&amp;gt;% 
    kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2336149&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3070802&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.7607617&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4467994&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;xTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1745982&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4256623&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7594604&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0057897&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(x ~ y, data = df, family = binomial()) %&amp;gt;% tidy %&amp;gt;% 
    kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4054651&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3227486&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.256288&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2090117&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;yTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1745982&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4256624&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.759460&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0057897&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$z = runif(100)
glm(y ~ x + z, data = df, family = binomial()) %&amp;gt;% 
    tidy %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.5930811&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5436028&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.0910191&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2752645&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;xTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2012521&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4293265&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7979921&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0051421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;z&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6601875&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8199692&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8051369&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4207407&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(x ~ y + z, data = df, family = binomial()) %&amp;gt;% 
    tidy %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1246648&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5141828&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2424523&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8084297&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;yTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1996170&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4291206&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7955243&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0051816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;z&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.5586854&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8023449&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.6963157&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4862311&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Adding another predictor doesn’t change the estimates either.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;Just to make sure I understand the estimates correctly. It represents the log odds ratio change for each “unit” of the predictor. In the case of a binary variable, the log odds ratio between the two groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(y ~ x, data = df, family = binomial()) %&amp;gt;% tidy %&amp;gt;% 
    kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2336149&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3070802&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.7607617&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4467994&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;xTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1745982&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4256623&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7594604&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0057897&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;odds.y.ifx = mean(subset(df, x)$y)/mean(!subset(df, 
    x)$y)
odds.y.ifnotx = mean(subset(df, !x)$y)/mean(!subset(df, 
    !x)$y)
log(odds.y.ifx/odds.y.ifnotx)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.174598&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;extreme-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extreme cases&lt;/h3&gt;
&lt;p&gt;How efficient is the logistic regression when there is an imbalance between different types of observations ? For example if just a few genomic regions overlap an interesting annotation and I want to test is the overlap is significant.&lt;/p&gt;
&lt;p&gt;Let’s look at the worst cases when there are only 1 observation for a particular class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df = data.frame(y = sample(c(FALSE, TRUE), 100, TRUE))
df$x = 1:nrow(df) %in% sample.int(nrow(df), 1)
glm(y ~ x, data = df, family = binomial()) %&amp;gt;% tidy %&amp;gt;% 
    kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1010961&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2012644&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5023050&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6154530&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;xTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.4649721&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1455.3975462&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0106259&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9915219&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Although the significance is low, the estimate seems quite high. I’ll repeat this process a bunch of time and with different number of supporting observations to have an idea of the distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ext.df = lapply(1:500, function(ii) {
    res = lapply(1:10, function(ssi) {
        df$x = 1:nrow(df) %in% sample.int(nrow(df), 
            ssi)
        glm(y ~ x, data = df, family = binomial()) %&amp;gt;% 
            tidy %&amp;gt;% mutate(rep = ii, ss = ssi)
    })
    do.call(rbind, res)
})
ext.df = do.call(rbind, ext.df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ext.df %&amp;gt;% filter(term == &amp;quot;xTRUE&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate)) + 
    geom_density(fill = &amp;quot;grey50&amp;quot;) + facet_grid(ss ~ 
    ., scales = &amp;quot;free&amp;quot;) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-16-Regression_files/figure-html/lrextsimgraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems like the estimate “inflation” is problematic mostly when there are only 1 or 2 supporting observations. If there are more than 5 supporting observations the estimate is correctly centered in 0.&lt;/p&gt;
&lt;p&gt;This problem is in fact called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Separation_(statistics)&#34;&gt;problem of separation&lt;/a&gt;. There are two approaches to deal with it:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Firth logistic regression.&lt;/li&gt;
&lt;li&gt;Exact logistic regression.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;a href=&#34;https://cran.r-project.org/web/packages/rms/index.html&#34;&gt;&lt;code&gt;rms&lt;/code&gt; package&lt;/a&gt; from &lt;a href=&#34;http://www.fharrell.com/2017/01/introduction.html&#34;&gt;Frank Harell&lt;/a&gt;. It implements a penalized maximum likelihood estimation of the model coefficients through the &lt;code&gt;lrm&lt;/code&gt; function which has a &lt;code&gt;penalty=&lt;/code&gt; parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rms)
extrms.df = lapply(1:200, function(ii) {
    res = lapply(1:10, function(ssi) {
        res = lapply(c(1, 3, 5), function(pen) {
            df$x = 1:nrow(df) %in% sample.int(nrow(df), 
                ssi)
            cc = lrm(y ~ x, data = df, penalty = pen)$coefficient
            data.frame(term = names(cc), estimate = cc, 
                rep = ii, ss = ssi, penalty = pen, 
                stringsAsFactors = FALSE)
        })
        do.call(rbind, res)
    })
    do.call(rbind, res)
})
extrms.df = do.call(rbind, extrms.df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extrms.df %&amp;gt;% filter(term == &amp;quot;x&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate)) + 
    geom_density(fill = &amp;quot;grey50&amp;quot;) + facet_grid(ss ~ 
    penalty, scales = &amp;quot;free&amp;quot;) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-16-Regression_files/figure-html/lrmsgraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It definitely helps: the estimates are now much closer to 0. I don’t see much difference between penalties 1, 3 or 5.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://cran.r-project.org/web/packages/logistf/index.html&#34;&gt;&lt;code&gt;logistf&lt;/code&gt; package&lt;/a&gt;. It implements Firth’s bias reduction method with its &lt;code&gt;logistf&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(logistf)
extstf.df = lapply(1:200, function(ii) {
    res = lapply(1:10, function(ssi) {
        df$x = 1:nrow(df) %in% sample.int(nrow(df), 
            ssi)
        cc = logistf(y ~ x, data = df)$coefficient
        data.frame(term = names(cc), estimate = cc, 
            rep = ii, ss = ssi, stringsAsFactors = FALSE)
    })
    do.call(rbind, res)
})
extstf.df = do.call(rbind, extstf.df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extstf.df %&amp;gt;% filter(term == &amp;quot;xTRUE&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate)) + 
    geom_density(fill = &amp;quot;grey50&amp;quot;) + facet_grid(ss ~ 
    ., scales = &amp;quot;free&amp;quot;) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-16-Regression_files/figure-html/lrreggraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This works well too.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;more-advanced-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More advanced models&lt;/h2&gt;
&lt;p&gt;A dummy example with some code for Generalized Additive Models, LOESS and SVM.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nb.samp = 1000
df = data.frame(x = runif(nb.samp, 0, 100))
df$y = rnorm(nb.samp, 0, 5) + abs(df$x - 25)
df$y = ifelse(df$x &amp;gt; 40, rnorm(nb.samp, 0, 5) - df$x * 
    df$x/300 + 20, df$y)
ggplot(df, aes(x = x, y = y)) + geom_point(alpha = 0.5) + 
    theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-16-Regression_files/figure-html/blm-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm.o = glm(y ~ x, data = df)
loess.o = loess(y ~ x, data = df)
library(mgcv)
gam.o = gam(y ~ s(x, bs = &amp;quot;cs&amp;quot;), data = df)
library(e1071)
svm.o = svm(y ~ x, data = df)

pred.df = rbind(df %&amp;gt;% mutate(y = predict(glm.o), model = &amp;quot;glm&amp;quot;), 
    df %&amp;gt;% mutate(y = predict(gam.o), model = &amp;quot;gam&amp;quot;), 
    df %&amp;gt;% mutate(y = predict(loess.o), model = &amp;quot;LOESS&amp;quot;), 
    df %&amp;gt;% mutate(y = predict(svm.o), model = &amp;quot;SVM&amp;quot;))

ggplot(df, aes(x = x, y = y)) + geom_point(alpha = 0.2) + 
    geom_line(aes(colour = model), size = 2, alpha = 0.9, 
        data = pred.df) + theme_bw() + scale_colour_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-16-Regression_files/figure-html/blmmodels-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Enrichment between genomic regions</title>
          <link>/Hippocamplus/2017/09/05/enrichment-between-genomic-regions/</link>
          <pubDate>Tue, 05 Sep 2017 00:00:00 UTC</pubDate>
          <author></author>
          <guid>/Hippocamplus/2017/09/05/enrichment-between-genomic-regions/</guid>
          <description>&lt;p&gt;Testing if two sets of genomic regions overlap significantly is not straightforward. In the simple situation of regions of 1 bp (e.g. SNVs) we could use a hypergeometric test. When the regions are small enough and there are not too many, the hypergeometric test might also be a fair approximation.&lt;/p&gt;
&lt;p&gt;But when we manipulate many regions of variable size covering the entire genome it’s not as straightforward. The gene annotation is an example. The repeat annotation is even worse as it covers almost 50% of the genome and contains different families with very different size/location profiles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr)
library(magrittr)
library(broom)
library(knitr)
library(tidyr)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;simulated-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulated data&lt;/h2&gt;
&lt;p&gt;In a very simple scenario of having only one chromosome of size 250 Mbp.&lt;/p&gt;
&lt;p&gt;First let’s create a function that draw random regions (ranges) in this chromosome.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(IRanges)
randRegions &amp;lt;- function(sizes, max.pos = 2.5e+08, max.iter = 10) {
    gr = IRanges(runif(length(sizes), 0, max.pos - 
        sizes), width = sizes)
    dup = which(countOverlaps(gr, gr) &amp;gt; 1)
    iter = 1
    while (iter &amp;lt;= max.iter &amp;amp; length(dup) &amp;gt; 0) {
        gr[dup] = IRanges(runif(length(dup), 0, max.pos - 
            sizes[dup]), width = sizes[dup])
        dup = which(countOverlaps(gr, gr) &amp;gt; 1)
    }
    return(gr)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now some regions will be our “repeats”: 10,000 regions from size 10 bp to 6 Kbp.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep.r = randRegions(runif(10000, 10, 6000))
sum(width(rep.r))/2.5e+08&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1196666&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They cover 11.97% of the chromosome.&lt;/p&gt;
&lt;p&gt;Now if we have another set of regions and we want to know how much they overlap with the repeats we could use the hypergeometric test. With this test we assume that we are sampling bases in the genome and testing if it’s covered by a repeat. In that sense, we expect 11.97% of our regions to overlap a repeat. If we compare random regions there shouldn’t be a significant overlap and the distribution of the P-value should be flat.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testHG &amp;lt;- function(feat.r, nb = 1000, size = 1, nb.test = 3000, 
    total.b = 2.5e+08) {
    exp.b = sum(width(feat.r))
    sapply(1:nb.test, function(ii) {
        reg.r = randRegions(rep(size, nb))
        obs.ol = sum(overlapsAny(reg.r, feat.r))
        phyper(obs.ol, exp.b, total.b - exp.b, length(reg.r), 
            lower.tail = FALSE)
    })
}

ht.sim = rbind(data.frame(nb = 1000, size = 1, pv = testHG(rep.r, 
    1000, 1)), data.frame(nb = 1000, size = 1000, pv = testHG(rep.r, 
    1000, 1000)), data.frame(nb = 100, size = 1000, 
    pv = testHG(rep.r, 100, 1000)), data.frame(nb = 1000, 
    size = 100, pv = testHG(rep.r, 1000, 100)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ht.sim %&amp;gt;% mutate(nbsize = paste0(nb, &amp;quot; x &amp;quot;, size, 
    &amp;quot;bp&amp;quot;)) %&amp;gt;% group_by(nbsize) %&amp;gt;% arrange(pv) %&amp;gt;% 
    mutate(cumprop = (1:n())/n()) %&amp;gt;% ggplot(aes(x = pv, 
    y = cumprop, color = nbsize)) + geom_line() + theme_bw() + 
    geom_abline(linetype = 2) + ylab(&amp;quot;cumulative proportion&amp;quot;) + 
    xlab(&amp;quot;P-value&amp;quot;) + scale_color_brewer(palette = &amp;quot;Set1&amp;quot;, 
    name = &amp;quot;regions&amp;quot;) + theme(legend.justification = c(1, 
    0), legend.position = c(0.99, 0.01))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-05-GenomicRegionEnrichment_files/figure-html/testhggraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, the hypergeometric test works well for region of 1 bp. Otherwise the distribution of the P-values is biased. The larger the regions the stronger the bias. To a lower extent, more regions also means more bias.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-control-regions-with-similar-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using control regions with similar features&lt;/h2&gt;
&lt;p&gt;We want to control for the size distribution and the total number of regions tested. Instead of the hypergeometric test, we can get control regions and compare their overlap with the actual regions, using a logistic regression for example. The control regions must be randomly distributed in the genome but have the same size distribution as our original regions. In the logistic regression we compare the two binary variables: overlapping a repeat or not, being an original region or a control region.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testLR &amp;lt;- function(feat.r, nb = 1000, size = 1, nb.test = 3000) {
    sapply(1:nb.test, function(ii) {
        reg.r = randRegions(rep(size, nb))
        cont.r = randRegions(width(reg.r))
        df = rbind(data.frame(region = TRUE, ol = overlapsAny(reg.r, 
            feat.r)), data.frame(region = FALSE, ol = overlapsAny(cont.r, 
            feat.r)))
        pvs = glm(ol ~ region, data = df, family = binomial()) %&amp;gt;% 
            tidy %&amp;gt;% .$p.value
        pvs[2]
    })
}

lr.sim = rbind(data.frame(nb = 1000, size = 1, pv = testLR(rep.r, 
    1000, 1)), data.frame(nb = 1000, size = 1000, pv = testLR(rep.r, 
    1000, 1000)), data.frame(nb = 100, size = 1000, 
    pv = testLR(rep.r, 100, 1000)), data.frame(nb = 1000, 
    size = 100, pv = testLR(rep.r, 1000, 100)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lr.sim %&amp;gt;% mutate(nbsize = paste0(nb, &amp;quot; x &amp;quot;, size, 
    &amp;quot;bp&amp;quot;)) %&amp;gt;% group_by(nbsize) %&amp;gt;% arrange(pv) %&amp;gt;% 
    mutate(cumprop = (1:n())/n()) %&amp;gt;% ggplot(aes(x = pv, 
    y = cumprop, color = nbsize)) + geom_line() + theme_bw() + 
    geom_abline(linetype = 2) + ylab(&amp;quot;cumulative proportion&amp;quot;) + 
    xlab(&amp;quot;P-value&amp;quot;) + scale_color_brewer(palette = &amp;quot;Set1&amp;quot;, 
    name = &amp;quot;regions&amp;quot;) + theme(legend.justification = c(1, 
    0), legend.position = c(0.99, 0.01))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-05-GenomicRegionEnrichment_files/figure-html/contreggraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distribution of the P-values is much better.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;controlling-for-correlated-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Controlling for correlated features&lt;/h2&gt;
&lt;p&gt;In the genome, the distribution of genes, repeats, functional regions, and others is not random. Different types of elements tend to be found together while others don’t. For example some repeats are located in GC-rich regions and others in AT-rich regions. Transposable elements don’t overlap exonic regions much. Their are hotspots of segmental duplications.&lt;/p&gt;
&lt;p&gt;Sometimes we want to control for the overlap with one (or more) genomic features to test the independent association of another. For example, we known copy number variants (CNVs) are enriched in segmental duplications and transposable elements are also enriched in segmental duplications. We might want to test if CNVs are independently enriched in regions with transposable elements, controlling for the overlap with segmental duplications.&lt;/p&gt;
&lt;p&gt;I tried to simulate a first set of regions that significantly overlaps our repeats and another one that significantly overlaps the first set. That way we should see a significant overlap with repeat when we test them separately, but the second one should be significant when we control for the first one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corRegions &amp;lt;- function(sizes, feat.r, or = 2, max.iter = 10, 
    max.pos = 2.5e+08) {
    reg.r = randRegions(sizes)
    for (ii in 1:or) {
        reg.r = c(reg.r[overlapsAny(reg.r, feat.r)], 
            randRegions(sizes))
    }
    dup = which(countOverlaps(reg.r, reg.r) &amp;gt; 1)
    sizes = width(reg.r)
    iter = 1
    while (iter &amp;lt;= max.iter &amp;amp; length(dup) &amp;gt; 0) {
        reg.r[dup] = IRanges(runif(length(dup), 0, 
            max.pos - sizes[dup]), width = sizes[dup])
        dup = which(countOverlaps(reg.r, reg.r) &amp;gt; 1)
    }
    reg.r
}

## First set of regions
repcor.r = corRegions(rep(10000, 1000), rep.r)
cont.r = randRegions(width(repcor.r))
df = rbind(data.frame(region = TRUE, ol = overlapsAny(repcor.r, 
    rep.r)), data.frame(region = FALSE, ol = overlapsAny(cont.r, 
    rep.r)))
glm(ol ~ region, data = df, family = binomial()) %&amp;gt;% 
    tidy %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2375323&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0464770&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-5.11075&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3e-07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;regionTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8549335&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0670786&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.74525&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Second set of regions
seed.r = randRegions(rep(10000, 6000))
seed.ol = overlapsAny(seed.r, repcor.r)
repcorcor.r = c(seed.r[seed.ol], seed.r[head(which(!seed.ol), 
    sum(!seed.ol) * 0.05)])
cont.r = randRegions(width(repcorcor.r))
df = rbind(data.frame(region = TRUE, ol = overlapsAny(repcorcor.r, 
    rep.r)), data.frame(region = FALSE, ol = overlapsAny(cont.r, 
    rep.r)))
glm(ol ~ region, data = df, family = binomial()) %&amp;gt;% 
    tidy %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2231436&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0590620&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-3.778125&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0001580&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;regionTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2214209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0832684&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.659123&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0078344&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;extending-the-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extending the logistic regression model&lt;/h3&gt;
&lt;p&gt;One strategy is to add a variable in the model that represents the effect we want to control for.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df = rbind(data.frame(region = TRUE, ol = overlapsAny(repcorcor.r, 
    rep.r), repcor = overlapsAny(repcorcor.r, repcor.r)), 
    data.frame(region = FALSE, ol = overlapsAny(cont.r, 
        rep.r), repcor = overlapsAny(cont.r, repcor.r)))
glm(ol ~ region + repcor, data = df, family = binomial()) %&amp;gt;% 
    tidy %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2817899&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0612109&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-4.6035936&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000042&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;regionTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0452678&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1093757&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4138742&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6789663&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;repcorTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4156934&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1096425&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.7913531&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0001498&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As expected, adding a variable in the model controls for this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;better-control-regions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Better control regions&lt;/h3&gt;
&lt;p&gt;Another approach is to control the specific overlap in the control regions. We want to force our control regions to overlap as much with the feature as the original regions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;randRegionsCons &amp;lt;- function(reg.r, feat.r, nb.seed = 1e+06) {
    seed.r = randRegions(rep(1, nb.seed))
    dist.df = distanceToNearest(seed.r, feat.r) %&amp;gt;% 
        as.data.frame
    reg.ol = overlapsAny(reg.r, feat.r)
    res.r = lapply(unique(width(reg.r)), function(size) {
        size.ii = which(width(reg.r) == size)
        res.r = IRanges()
        if (sum(reg.ol[size.ii]) &amp;gt; 0) {
            seed.ii = dist.df %&amp;gt;% filter(distance &amp;lt; 
                size/2) %&amp;gt;% .$queryHits %&amp;gt;% sample(sum(reg.ol[size.ii]))
            res.r = c(res.r, resize(seed.r[seed.ii], 
                size, fix = &amp;quot;center&amp;quot;))
        }
        if (sum(!reg.ol[size.ii]) &amp;gt; 0) {
            seed.ii = dist.df %&amp;gt;% filter(distance &amp;gt; 
                size/2) %&amp;gt;% .$queryHits %&amp;gt;% sample(sum(!reg.ol[size.ii]))
            res.r = c(res.r, resize(seed.r[seed.ii], 
                size, fix = &amp;quot;center&amp;quot;))
        }
        res.r
    })
    do.call(c, res.r)
}

contSize.r = randRegionsCons(repcorcor.r, repcor.r)
df = rbind(data.frame(region = TRUE, ol = overlapsAny(repcorcor.r, 
    rep.r), repcor = overlapsAny(repcorcor.r, repcor.r)), 
    data.frame(region = FALSE, ol = overlapsAny(contSize.r, 
        rep.r), repcor = overlapsAny(contSize.r, repcor.r)))
glm(ol ~ region, data = df, family = binomial()) %&amp;gt;% 
    tidy %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1709578&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0589111&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.901961&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0037083&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;regionTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1726805&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0831615&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.076448&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0378526&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It works too. One benefit of this approach is its interpretability: we can directly compare summary metrics using the control regions, e.g. like the proportion of regions overlapping repeats.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(overlapsAny(repcorcor.r, repcor.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7812231&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(overlapsAny(cont.r, repcor.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1395349&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(overlapsAny(contSize.r, repcor.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7812231&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This could be useful in situation with extreme overlap distribution. If only a few regions overlap the feature in the simple control regions, the regression might not correct for it as well as if forcing the control regions to be similar. Maybe there would some differences in power in those cases ?&lt;/p&gt;
&lt;p&gt;However building these control regions can become computationally intense, especially if the sizes of the regions vary and several features need to be controlled.&lt;/p&gt;
&lt;p&gt;In practice I would do both: include the variable in the regression model and use regions with controlled overlap.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-different-sets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing different sets&lt;/h2&gt;
&lt;p&gt;What if we need to compare sets of regions &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; with a third one &lt;em&gt;C&lt;/em&gt;. If the &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; are comparable in term of size and total number we could directly compare the overlap or an enrichment estimate (e.g. model estimate). If &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; have different size distribution or just total number of regions, these estimates may not be directly comparable. If they both overlap significantly with &lt;em&gt;C&lt;/em&gt;, the previous test (control regions + logistic regression) should test them significant. Even the P-value might be affected by the difference in size/number between the two sets. But how should we compared them ? Which interpretable metric could we use to compare enrichment of two different sets or regions ?&lt;/p&gt;
&lt;p&gt;A practical example would be two catalogs of CNVs, say from two different methods, that we want to compare to a functional annotation. If one catalogs has more CNVs, or has larger CNVs, how can we say which one overlaps better with the functional annotation ?&lt;/p&gt;
&lt;p&gt;I simulate this scenario and compare a few metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The fold-change in overlap proportion: proportion overlapping / proportion overlapping in control regions.&lt;/li&gt;
&lt;li&gt;The diff-change in overlap proportion: proportion overlapping - proportion overlapping in control regions.&lt;/li&gt;
&lt;li&gt;The logistic regression estimate which are log odds ratio.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Well, the main question was how should I simulate this. I ended up simulating two sets with similar odds ratio so we already know which metric will work better… One of the value of simulation is to force us to define the question. Or at least think about it. In this example, forcing two different sets to have similar odds ratio seemed more natural than trying to double the proportion for example. The odds ratio seems more fair to me and might avoid the situation where we are more likely to observe a large effect size just because the regions are rarer/smaller.&lt;/p&gt;
&lt;p&gt;Using a set of functional regions, I will try to compare a set of small CNVs and large CNVs. We expect more of the large CNVs to overlap the functional regions by chance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fun.r = randRegions(rep(10, 30000))
cnv.sm = randRegions(rep(1000, 1000))
cnv.lg = randRegions(rep(10000, 1000))
mean(overlapsAny(cnv.sm, fun.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.112&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(overlapsAny(cnv.lg, fun.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.688&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testLR &amp;lt;- function(reg.r, feat.r) {
    cont.r = randRegions(width(reg.r))
    df.sm = rbind(data.frame(region = TRUE, ol = overlapsAny(reg.r, 
        feat.r)), data.frame(region = FALSE, ol = overlapsAny(cont.r, 
        feat.r)))
    rbind(data.frame(term = &amp;quot;fold-change&amp;quot;, estimate = mean(overlapsAny(reg.r, 
        feat.r))/mean(overlapsAny(cont.r, feat.r)), 
        p.value = NA), data.frame(term = &amp;quot;diff-change&amp;quot;, 
        estimate = mean(overlapsAny(reg.r, feat.r)) - 
            mean(overlapsAny(cont.r, feat.r)), p.value = NA), 
        glm(ol ~ region, data = df.sm, family = binomial()) %&amp;gt;% 
            tidy %&amp;gt;% select(term, estimate, p.value))
}

metrics.df = rbind(testLR(cnv.sm, fun.r) %&amp;gt;% mutate(region = &amp;quot;cnv.sm&amp;quot;), 
    testLR(cnv.lg, fun.r) %&amp;gt;% mutate(region = &amp;quot;cnv.lg&amp;quot;))
metrics.df %&amp;gt;% filter(term != &amp;quot;(Intercept)&amp;quot;) %&amp;gt;% select(region, 
    term, estimate) %&amp;gt;% kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;region&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cnv.sm&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;fold-change&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0566038&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cnv.sm&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;diff-change&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0060000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cnv.sm&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;regionTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0617938&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cnv.lg&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;fold-change&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9745042&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cnv.lg&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;diff-change&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0180000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cnv.lg&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;regionTRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0852498&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;no-association&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;No association&lt;/h3&gt;
&lt;p&gt;If the CNVs are not enriched in the functional regions, how do the three metrics compare ?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;null.df = lapply(1:1000, function(ii) {
    cnv.sm = randRegions(rep(1000, 1000))
    cnv.lg = randRegions(rep(10000, 1000))
    rbind(testLR(cnv.sm, fun.r) %&amp;gt;% mutate(region = &amp;quot;cnv.sm&amp;quot;, 
        rep = ii), testLR(cnv.lg, fun.r) %&amp;gt;% mutate(region = &amp;quot;cnv.lg&amp;quot;, 
        rep = ii))
})
null.df = do.call(rbind, null.df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;null.df %&amp;gt;% filter(term == &amp;quot;regionTRUE&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate, 
    colour = region)) + geom_density() + theme_bw() + 
    geom_vline(xintercept = 0) + ggtitle(&amp;quot;log odds ratio&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-05-GenomicRegionEnrichment_files/figure-html/cnvnoassocgraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;null.df %&amp;gt;% filter(term == &amp;quot;fold-change&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate, 
    colour = region)) + geom_density() + theme_bw() + 
    geom_vline(xintercept = 1) + ggtitle(&amp;quot;fold-change&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-05-GenomicRegionEnrichment_files/figure-html/cnvnoassocgraph-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;null.df %&amp;gt;% filter(term == &amp;quot;diff-change&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate, 
    colour = region)) + geom_density() + theme_bw() + 
    geom_vline(xintercept = 0) + ggtitle(&amp;quot;diff-change&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-05-GenomicRegionEnrichment_files/figure-html/cnvnoassocgraph-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The three metrics are centered in 0 but the variance of the fold-change metric is much higher for the small CNVs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;association&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Association&lt;/h3&gt;
&lt;p&gt;If the odds of overlapping the functional regions are doubled.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;asso.df = lapply(1:1000, function(ii) {
    cnv.sm = randRegions(rep(1000, 1000))
    cnv.lg = randRegions(rep(10000, 1000))
    cnv.sm = c(cnv.sm[overlapsAny(cnv.sm, fun.r)], 
        randRegions(rep(1000, 1000)))
    cnv.lg = c(cnv.lg[overlapsAny(cnv.lg, fun.r)], 
        randRegions(rep(10000, 1000)))
    rbind(testLR(cnv.sm, fun.r) %&amp;gt;% mutate(region = &amp;quot;cnv.sm&amp;quot;, 
        rep = ii), testLR(cnv.lg, fun.r) %&amp;gt;% mutate(region = &amp;quot;cnv.lg&amp;quot;, 
        rep = ii))
})
asso.df = do.call(rbind, asso.df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;asso.df %&amp;gt;% filter(term == &amp;quot;regionTRUE&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate, 
    colour = region)) + geom_density() + theme_bw() + 
    geom_vline(xintercept = 0) + ggtitle(&amp;quot;log odds ratio&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-05-GenomicRegionEnrichment_files/figure-html/cnvassocgraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;asso.df %&amp;gt;% filter(term == &amp;quot;fold-change&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate, 
    colour = region)) + geom_density() + theme_bw() + 
    geom_vline(xintercept = 1) + ggtitle(&amp;quot;fold-change&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-05-GenomicRegionEnrichment_files/figure-html/cnvassocgraph-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;asso.df %&amp;gt;% filter(term == &amp;quot;diff-change&amp;quot;) %&amp;gt;% ggplot(aes(x = estimate, 
    colour = region)) + geom_density() + theme_bw() + 
    geom_vline(xintercept = 0) + ggtitle(&amp;quot;diff-change&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-09-05-GenomicRegionEnrichment_files/figure-html/cnvassocgraph-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected by construction, only the logistic regression estimate are similar. If we used the fold-change metric it would look like the small CNVs are more enriched; with the diff-change metric the large CNVs would.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
        </item>
      
    

  </channel>
</rss>

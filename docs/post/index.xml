<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>/Hippocamplus/post/</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 17 Nov 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/Hippocamplus/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Speeding up blogdown/Pandoc for large bibliography</title>
      <link>/Hippocamplus/2018/11/17/speedup-blogwdown-pandoc-large-bibliography/</link>
      <pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/11/17/speedup-blogwdown-pandoc-large-bibliography/</guid>
      <description>I have another website where I write down my reviews of the papers I read. To handle citations in pages and posts, I was originally using jekyll-scholar. It scales well enough that I could have one main BibTeX file for all the pages of the website. I’m now switching to blogdown/Hugo because it’s apparently faster, with less dependencies, but most importantly because it’s very easy to integrate R code with RMarkdown.</description>
    </item>
    
    <item>
      <title>The Formation of the Scientific Mind, Gaston Bachelard - part 1</title>
      <link>/Hippocamplus/2018/11/12/bachelard-formation-esprit-scientifique-part1/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/11/12/bachelard-formation-esprit-scientifique-part1/</guid>
      <description>Quotes about science The concept of epistemological obstacle The first obstacle: the primary experiment General knowledge as an obstacle An example of verbal obstacle: the sponge Aside on education Sick burns   These are notes from my reading of La formation de l’esprit scientifique by Gaston Bachelard. I hesitated a bit but finally decided to write the notes in English. In the end this blog is as much a place to save things I want to remember as a place to practice writing in English and share with other people.</description>
    </item>
    
    <item>
      <title>Peer-review opportunities for early career researchers</title>
      <link>/Hippocamplus/2018/11/07/ecr-peerreview/</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/11/07/ecr-peerreview/</guid>
      <description>Some notes from today’s workshop organized by USPA with bio-protocol and eLife ambassadors.
Bio-protocol Data is uploaded to databases, code to repos like GitHub but protocols are still just in the Methods section of papers. And the Methods of a paper often omits many details which makes it difficult to reuse the experiment or reproduce results.
Dennis Bua introduced bio-protocol, a platform to peer-review protocols. Here the goal is to make sure all the information is provided to reproduce an experiment.</description>
    </item>
    
    <item>
      <title>Syncing Mendeley and PDFs across devices</title>
      <link>/Hippocamplus/2018/09/22/sync-mendeley/</link>
      <pubDate>Sat, 22 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/09/22/sync-mendeley/</guid>
      <description>Recently, I’ve been setting up new computers from scratch as I moved from Montreal to Santa Cruz. Like for spring cleaning, it might be a good idea to clarify the system I’ve been using to manage bibliography and PDF annotation.
Briefly I use Mendeley Desktop and also put all PDF files in a Google Drive. Both are synced with my Android tablet. On the tablet, I use the Mendeley app to get information, search etc, but I read/annotate the PDFs from the Google Drive.</description>
    </item>
    
    <item>
      <title>Mental health crisis in science...but careful with nonresponse bias</title>
      <link>/Hippocamplus/2018/07/08/mental-health-crisis-science/</link>
      <pubDate>Sun, 08 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/07/08/mental-health-crisis-science/</guid>
      <description>A few month ago, a short paper was published in Nature Biotechnology1 about the mental health crisis in science. It attracted a bit of attention on the news and social media, which is good because it’s an important matter. The article was very good at putting the subject on the table and proposing some solutions, but it picked my curiosity about nonresponse bias.
Issues with the Nature Biotech article The numbers are based on an email survey so one issue is the nonresponse bias: the individuals that responded might not be representative of the population.</description>
    </item>
    
    <item>
      <title>Clustering into same size clusters</title>
      <link>/Hippocamplus/2018/06/09/cluster-same-size/</link>
      <pubDate>Sat, 09 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/06/09/cluster-same-size/</guid>
      <description>Methods Iterative dichotomy Iterative nearest neighbor Same-size k-Means Variation Iterative “bottom-leaves” hierarchical clustering  Test data Results Cluster size Within-cluster distance Silhouette score  Conclusions Extra: optimization Code   Update Nov 23 2018: New iterative approach using hierarchical clustering and better graphs.
I would like to cluster points into groups of similar size. For example I would like to group 1000 points into clusters of around 10 points each.</description>
    </item>
    
    <item>
      <title>Converting scientific reviews to EPUB</title>
      <link>/Hippocamplus/2018/05/07/epub-reviews/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/05/07/epub-reviews/</guid>
      <description>First, check on PubMed Central Convert the HTML page to EPUB Clean up the HTML before conversion Compiling several reviews into one EPUB document VPN, paywall and Pandoc Limitations Methods  Other EPUB resources   Third post on the series of “Things I did instead of writing my thesis to help me write my thesis”: how to find/convert reviews in the EPUB format to read in an ebook reader.</description>
    </item>
    
    <item>
      <title>Additional checks for a LaTeX manuscript</title>
      <link>/Hippocamplus/2018/04/18/check-latex-pub/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/04/18/check-latex-pub/</guid>
      <description>To continue on the series of “Things I did instead of writing my thesis to help me write my thesis”, another Python script that reads a LaTeX manuscript and helps check that everything is fine. More specifically, the checkLatex.py script (on GitHub) will:
List missing references. List multi-references to reorder. List duplicated labels. List labels that don’t start by fig: or tab:. List figures/tables that are not in order. List ?</description>
    </item>
    
    <item>
      <title>Checking text similarity between two documents</title>
      <link>/Hippocamplus/2018/04/16/text-similarity/</link>
      <pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/04/16/text-similarity/</guid>
      <description>To start the series of “Things I did instead of writing my thesis to help me write my thesis”, a small Python script that compares two text documents and output similar parts. I did that to avoid auto-plagiarism of my manuscripts’ introduction in the main thesis introduction.
It’s a very naive approach but sped up the checking process (maybe worth the time). It first looks for short exact matches between the two documents, then extends these exact matches and uses the difflib module to keep text with a minimum similarity score (default 80%).</description>
    </item>
    
    <item>
      <title>Journal comparison</title>
      <link>/Hippocamplus/2018/02/23/journals-comparison/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/02/23/journals-comparison/</guid>
      <description>Edit Feb 24: Added variance graph and some examples of suspiciously fast publications.
Edit Feb 25: Added script and data to GitHub.
Some info about journals in my field.
Summary table   Journal Co. IF OA APC Other fees Pub/year Received-to-accepted in days. median (75th perc.)    F1000Research - 1.2 Y 1000 USD -    PeerJ - 2.2 Y 1095 USD - ~1290 ~88 (139)  eLife - 7.</description>
    </item>
    
    <item>
      <title>tSNE and clustering</title>
      <link>/Hippocamplus/2018/02/13/tsne-and-clustering/</link>
      <pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2018/02/13/tsne-and-clustering/</guid>
      <description>tSNE can give really nice results when we want to visualize many groups of multi-dimensional points. Once the 2D graph is done we might want to identify which points cluster in the tSNE blobs.
Using simulated and real data, I’ll try different methods:
 Hierarchical clustering K-means Gaussian mixture Density-based clustering Louvain community detection.  TL;DR If &amp;lt;30K points, hierarchical clustering is robust, easy to use and with reasonable computing time.</description>
    </item>
    
    <item>
      <title>Bibliography style for AJHG</title>
      <link>/Hippocamplus/2017/10/04/bibliography-style-for-ajhg/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2017/10/04/bibliography-style-for-ajhg/</guid>
      <description>I couldn’t find an up-to-date/working LaTeX bibliography style for the American Journal of Human Genetics (AJHG). The output from unsrtnat (my goto style) was also quite different from what the journal wanted.
I found a bibliography style for Cell which is almost what AJHG wants, but I also wanted the references to be ordered by their appearance in the text (like for unsrtnat) and not alphabetically. So I downloaded both cell.</description>
    </item>
    
    <item>
      <title>MUMmerplots with ggplot2</title>
      <link>/Hippocamplus/2017/09/19/mummerplots-with-ggplot2/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2017/09/19/mummerplots-with-ggplot2/</guid>
      <description>Update Oct 28 2018: added reference id (rid) to be able to visualize multiple reference regions. Also uploaded the example data somewhere.
library(dplyr) library(magrittr) library(GenomicRanges) library(knitr) library(ggplot2) library(tidyr) MUMmer plot The MUMmer plot that I want to reproduce showed three contigs overlapping a region of chr 14. I had filtered the delta file with delta-filter -l 10000 -q -r to get only the contigs with the best alignments. I had used mummerplot with the -l layout option to reorder and orient the sequences to have a nice diagonal.</description>
    </item>
    
    <item>
      <title>Regression sandbox</title>
      <link>/Hippocamplus/2017/09/16/regression-sandbox/</link>
      <pubDate>Sat, 16 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2017/09/16/regression-sandbox/</guid>
      <description>library(ggplot2) library(broom) library(magrittr) library(dplyr) library(knitr) Logistic regression One way or another If we have two binary variables and we want to see if they are associated we could use a logistic regression. How do we decide which variable to be the predictor and which variable to observed variable ?
In theory there shouldn’t be any differences but let’s check with a dummy example:
df = data.frame(x = sample(c(FALSE, TRUE), 100, TRUE)) df$y = df$x df$y[1:70] = sample(c(FALSE, TRUE), 70, TRUE) glm(y ~ x, data = df, family = binomial()) %&amp;gt;% tidy %&amp;gt;% kable   term estimate std.</description>
    </item>
    
    <item>
      <title>Enrichment between genomic regions</title>
      <link>/Hippocamplus/2017/09/05/enrichment-between-genomic-regions/</link>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2017/09/05/enrichment-between-genomic-regions/</guid>
      <description>Testing if two sets of genomic regions overlap significantly is not straightforward. In the simple situation of regions of 1 bp (e.g. SNVs) we could use a hypergeometric test. When the regions are small enough and there are not too many, the hypergeometric test might also be a fair approximation.
But when we manipulate many regions of variable size covering the entire genome it’s not as straightforward. The gene annotation is an example.</description>
    </item>
    
    <item>
      <title>Segmental duplication exploration</title>
      <link>/Hippocamplus/2016/10/20/segmental-duplication-exploration/</link>
      <pubDate>Thu, 20 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2016/10/20/segmental-duplication-exploration/</guid>
      <description>Segmental Duplications (SD) I downloaded the segmental duplication annotation for hg19 from UCSC. There are 51599 annotated SD. They are defined as regions larger than 1 Kbp with at least 90% similarity with another region in the genome.
 Segmental duplication regions Many SD are nested of located next to each other. I merge overlapping SDs (or located at &amp;lt;10 bp) to create SD regions, i.e. longer stretch of the genome overlapping SDs.</description>
    </item>
    
    <item>
      <title>Summary epigenetic mark tracks</title>
      <link>/Hippocamplus/2016/09/06/summary-epigenetic-mark-tracks/</link>
      <pubDate>Tue, 06 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2016/09/06/summary-epigenetic-mark-tracks/</guid>
      <description>To assess the potential impact of variants (SNV, SVs) we might want to use some of the public epigentic datasets. The amount and heterogeneity of this data is a bit overwhelming. I would like to get a summary of which regions of the genome are the most functionally important.
The plan is to:
 get annotated peaks for the 6 typical histone marks in 5-6 tissues, merging sub-tissues (e.g. brain subregions) keep regions supported by enough replicates  Eventually, I could also annotate the regions that are tissue-specific or shared across tissues.</description>
    </item>
    
    <item>
      <title>Gencode exploration</title>
      <link>/Hippocamplus/2016/06/04/gencode-exploration/</link>
      <pubDate>Sat, 04 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2016/06/04/gencode-exploration/</guid>
      <description>Gencode v19 I downloaded Gencode v19 at ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_19/gencode.v19.annotation.gtf.gz.
 Genes Number Focusing on autosomes/X/Y, there are 57,783 “genes” of different types:
I merge the rare types into a other class and some RNAs.
  gene_type.f n    protein_coding 20332  pseudogene 13931  other 7417  lincRNA 7114  RNA 5934  miRNA 3055     Size The largest annotated genes span more than 2 Mbp:</description>
    </item>
    
    <item>
      <title>Preparing some genomic annotations</title>
      <link>/Hippocamplus/2016/06/03/preparing-some-genomic-annotations/</link>
      <pubDate>Fri, 03 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2016/06/03/preparing-some-genomic-annotations/</guid>
      <description>Mappability track I produced a mappability track from the UCSC track. The raw file contains, for each base in the genome, an estimation of the probability that a read is correctly mapped at this position.
Using a sliding-window approach, I compute the average mappability in regions of size 1 Kbp. This is a more manageable amount of data and still informative, especially when interested in large regions (e.g. SVs).
I used a custom Perl script to efficiently parse the bedGraph-transformed original file.</description>
    </item>
    
    <item>
      <title>Word Cloud in R</title>
      <link>/Hippocamplus/2016/02/26/word-cloud-in-r/</link>
      <pubDate>Fri, 26 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/Hippocamplus/2016/02/26/word-cloud-in-r/</guid>
      <description>The wordcloud package is available on CRAN.
Fake words I create fake words to see a bit how the command is working.
library(wordcloud) createWords &amp;lt;- function(w.l = 3) paste(sample(letters, w.l, TRUE), collapse = &amp;quot;&amp;quot;) words = sapply(1:200, function(e) createWords(runif(1, 3, 10))) freq = c(sample(1:30, 190, T), sample(30:150, 10, T)) freq = freq/sum(freq) wordcloud(words, freq) ## Big words in the center wordcloud(words, freq, random.order = FALSE) ## Max word number wordcloud(words, freq, max.</description>
    </item>
    
  </channel>
</rss>